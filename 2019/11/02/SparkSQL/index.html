<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="后端Java工程师"><title>SparkSQL | dian的博客</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 4.2.1"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">SparkSQL</h1><a id="logo" href="/.">dian的博客</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">SparkSQL</h1><div class="post-meta">2019-11-02</div><div class="post-content"><a id="more"></a>

<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Spark SQL是Spark用来处理结构化数据的一个模块，它提供了2个编程抽象：<strong>DataFrame</strong>和<strong>DataSet</strong>，并且作为分布式SQL查询引擎的作用。</p>
<p>将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！</p>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ol>
<li>易整合</li>
<li>统一的数据访问方式</li>
<li>兼容Hive</li>
<li>标准的数据连接</li>
</ol>
<h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><p>DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息</p>
<p><img src="/2019/11/02/SparkSQL/RDD&DataFrame.png" alt></p>
<p>可以把它当做数据库中的一张表来对待，DataFrame也是懒执行的。性能上比RDD要高，</p>
<p>优化的执行计划：查询计划通过Spark catalyst optimiser进行优化。</p>
<h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为Dataset。DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person].DataFrame只是知道字段，但是不知道字段的类型。</p>
<blockquote>
<p>在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。</p>
</blockquote>
<h2 id="SparkSQL编程"><a href="#SparkSQL编程" class="headerlink" title="SparkSQL编程"></a>SparkSQL编程</h2><h2 id="SparkSession新的起始点"><a href="#SparkSession新的起始点" class="headerlink" title="SparkSession新的起始点"></a>SparkSession新的起始点</h2><p>在老的版本中，SparkSQL提供两种SQL查询起始点：一个叫SQLContext，用于Spark自己提供的SQL查询；一个叫HiveContext，用于连接Hive的查询。</p>
<p>SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。</p>
<h3 id="DataFrame创建使用"><a href="#DataFrame创建使用" class="headerlink" title="DataFrame创建使用"></a>DataFrame创建使用</h3><h4 id="SQL风格"><a href="#SQL风格" class="headerlink" title="SQL风格"></a>SQL风格</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//读取json，展示</span></span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">"/opt/module/spark/examples/src/main/resources/people</span></span><br><span class="line"><span class="string">df.show</span></span><br><span class="line"><span class="string">/*SQL风格语法*/</span></span><br><span class="line"><span class="string">//创建临时表</span></span><br><span class="line"><span class="string">df.createOrReplaceTempView("</span><span class="string">people")</span></span><br><span class="line"><span class="string">//sql查询</span></span><br><span class="line"><span class="string">val sqlDF = spark.sql("</span><span class="type">SELECT</span> * <span class="type">FROM</span> <span class="string">people")</span></span><br><span class="line"><span class="string">sqlDF.show</span></span><br><span class="line"><span class="string">//临时表是Session范围内的，Session退出后，表就失效了。如果想应用范围内有效，可以使用全局表。注意使用全局表时需要全路径访问，如：global_temp.people</span></span><br><span class="line"><span class="string">df.createGlobalTempView("</span><span class="string">people")</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>临时表是Session范围内的，Session退出后，表就失效了。</p>
</blockquote>
<h4 id="DSL风格语法-次要"><a href="#DSL风格语法-次要" class="headerlink" title="DSL风格语法(次要)"></a>DSL风格语法(次要)</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.read</span><br><span class="line">df.printSchema</span><br><span class="line">df.select(<span class="string">"name"</span>).show()</span><br><span class="line"><span class="comment">//查看”name”列数据以及”age+1”数据，要用转换符</span></span><br><span class="line">df.select($<span class="string">"name"</span>, $<span class="string">"age"</span> + <span class="number">1</span>).show()</span><br><span class="line">df.groupBy(<span class="string">"age"</span>).count().show()</span><br></pre></td></tr></table></figure>

<p>导入的是对象的包</p>
<h2 id="DateFrame、RDD和DataSet相互转化"><a href="#DateFrame、RDD和DataSet相互转化" class="headerlink" title="DateFrame、RDD和DataSet相互转化"></a>DateFrame、RDD和DataSet相互转化</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//导入隐式转换并创建一个RDD</span></span><br><span class="line"><span class="comment">//spark不是包名，而是sparkSession对象的名称</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="comment">//RDD转换为DateFrame</span></span><br><span class="line"><span class="comment">//Shell</span></span><br><span class="line"><span class="comment">//1</span></span><br><span class="line"><span class="keyword">val</span> peopleRDD = sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line">peopleRDD.map&#123;x=&gt;<span class="keyword">val</span> para = x.split(<span class="string">","</span>);(para(<span class="number">0</span>),para(<span class="number">1</span>).trim.toInt)&#125;.toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line"><span class="comment">//2</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">peopleRDD</span>.<span class="title">map</span></span>&#123; x =&gt; <span class="keyword">val</span> para = x.split(<span class="string">","</span>);<span class="type">People</span>(para(<span class="number">0</span>),para(<span class="number">1</span>).trim.toInt)&#125;.toDF</span><br><span class="line"><span class="comment">//DateFrame转换为RDD</span></span><br><span class="line"><span class="comment">//直接.rdd即可</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//DataSet创建</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Andy"</span>, <span class="number">32</span>)).toDS()</span><br><span class="line"><span class="comment">//RDD转换为DataSet</span></span><br><span class="line"><span class="keyword">val</span> peopleRDD = sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">peopleRDD</span>.<span class="title">map</span>(<span class="params">line =&gt; &#123;val para = line.split(","</span>)</span>;<span class="type">Person</span>(para(<span class="number">0</span>),para(<span class="number">1</span>).trim.toInt)&#125;).toDS()</span><br><span class="line"></span><br><span class="line"><span class="comment">//DataSet转换为RDD</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">DS</span> = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Andy"</span>, <span class="number">32</span>)).toDS()</span><br><span class="line"><span class="type">DS</span>.rdd</span><br><span class="line"></span><br><span class="line"><span class="comment">//DataFrame转换为DataSet</span></span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">df</span>.<span class="title">as</span>[<span class="type">Person</span>]</span></span><br><span class="line"><span class="class"><span class="title">//DataSet转换为DataFrame</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">ds</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Andy"</span>, <span class="number">32</span>)).toDS()</span><br><span class="line"><span class="keyword">val</span> df = ds.toDF</span><br><span class="line"></span><br><span class="line"><span class="comment">//DataSet转DataFrame</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> testDF = testDS.toDF</span><br><span class="line"><span class="comment">// DataFrame转DataSet</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">testDS</span> </span>= testDF.as[<span class="type">Coltest</span>]</span><br></pre></td></tr></table></figure>

<h3 id="三者的共性"><a href="#三者的共性" class="headerlink" title="三者的共性"></a>三者的共性</h3><ol>
<li><p>RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利</p>
</li>
<li><p>三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算。</p>
</li>
<li><p>三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出。</p>
</li>
<li><p>三者都有partition的概念</p>
</li>
<li><p>三者有许多共同的函数，如filter，排序等</p>
</li>
<li><p>在对DataFrame和Dataset进行操作许多操作都需要这个包进行支持<code>import spark.implicits._</code></p>
</li>
<li><p>DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型</p>
</li>
</ol>
<h3 id="三者的区别"><a href="#三者的区别" class="headerlink" title="三者的区别"></a>三者的区别</h3><ol>
<li><p>RDD:</p>
<ul>
<li><p>RDD一般和spark mlib(机器学习)同时使用</p>
</li>
<li><p>RDD不支持sparkSQL操作</p>
</li>
</ul>
</li>
<li><p>DataFrame:</p>
<ul>
<li><p>与和不同，每一行的类型固定为，每一列的值没法直接访问，只有通过解析才能获取各个字段的值</p>
</li>
<li><p>DataFrame与Dataset一般不与spark mlib同时使用</p>
</li>
<li><p>DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作</p>
</li>
<li><p>DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//保存</span></span><br><span class="line"><span class="keyword">val</span> saveoptions = <span class="type">Map</span>(<span class="string">"header"</span> -&gt; <span class="string">"true"</span>, <span class="string">"delimiter"</span> -&gt; <span class="string">"\t"</span>, <span class="string">"path"</span> -&gt; <span class="string">"hdfs://hadoop102:9000/test"</span>)</span><br><span class="line">datawDF.write.format(<span class="string">"com.atguigu.spark.csv"</span>).mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).options(saveoptions).save()</span><br><span class="line"><span class="comment">//读取</span></span><br><span class="line"><span class="keyword">val</span> options = <span class="type">Map</span>(<span class="string">"header"</span> -&gt; <span class="string">"true"</span>, <span class="string">"delimiter"</span> -&gt; <span class="string">"\t"</span>, <span class="string">"path"</span> -&gt; <span class="string">"hdfs://hadoop102:9000/test"</span>)</span><br><span class="line"><span class="keyword">val</span> datarDF= spark.read.options(options).format(<span class="string">"com.atguigu.spark.csv"</span>).load()</span><br></pre></td></tr></table></figure>
</li>
<li><p>Dataset:</p>
<ul>
<li><p>Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。</p>
</li>
<li><p>DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息</p>
</li>
</ul>
</li>
</ol>
<h3 id="IDEA创建SparkSQL程序"><a href="#IDEA创建SparkSQL程序" class="headerlink" title="IDEA创建SparkSQL程序"></a>IDEA创建SparkSQL程序</h3><p>pom文件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.<span class="type">LoggerFactory</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">//创建SparkConf()并设置App名称</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="comment">// For implicit conversions like converting RDDs to DataFrames</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"data/people.json"</span>)</span><br><span class="line">    <span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">    df.show()</span><br><span class="line">    df.filter($<span class="string">"age"</span> &gt; <span class="number">21</span>).show()</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"persons"</span>)</span><br><span class="line">    spark.sql(<span class="string">"SELECT * FROM persons where age &gt; 21"</span>).show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h2><p><code>spark.udf.register(&quot;addName&quot;, (x:String)=&gt;&quot;Name:&quot;+x)</code></p>
</div><div class="tags"><a href="/tags/Spark/"><i class="fa fa-tag"></i>Spark</a><a href="/tags/SQL/"><i class="fa fa-tag"></i>SQL</a><a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"><i class="fa fa-tag"></i>大数据</a></div><div class="post-nav"><a class="pre" href="/2019/11/08/Vue%E7%AC%94%E8%AE%B0/">Vue笔记</a><a class="next" href="/2019/10/30/SparkCore/">SparkCore</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://example.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/%E5%B9%B6%E5%8F%91/" style="font-size: 15px;">并发</a> <a href="/tags/nosql/" style="font-size: 15px;">nosql</a> <a href="/tags/elasticsearch/" style="font-size: 15px;">elasticsearch</a> <a href="/tags/redis/" style="font-size: 15px;">redis</a> <a href="/tags/%E5%90%8E%E7%AB%AF/" style="font-size: 15px;">后端</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/PHP/" style="font-size: 15px;">PHP</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/SQL/" style="font-size: 15px;">SQL</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 15px;">大数据</a> <a href="/tags/scala/" style="font-size: 15px;">scala</a> <a href="/tags/php/" style="font-size: 15px;">php</a> <a href="/tags/wordpress/" style="font-size: 15px;">wordpress</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/springsecurity/" style="font-size: 15px;">springsecurity</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/bug/" style="font-size: 15px;">bug</a> <a href="/tags/marjora/" style="font-size: 15px;">marjora</a> <a href="/tags/virtualbox/" style="font-size: 15px;">virtualbox</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 15px;">服务器</a> <a href="/tags/C/" style="font-size: 15px;">C++</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 15px;">博客</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" style="font-size: 15px;">计算机基础</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 15px;">设计模式</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 15px;">爬虫</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">并发编程</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a> <a href="/tags/spring/" style="font-size: 15px;">spring</a> <a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 15px;">前端</a> <a href="/tags/Vue/" style="font-size: 15px;">Vue</a> <a href="/tags/%E6%B5%81%E7%A8%8B/" style="font-size: 15px;">流程</a> <a href="/tags/%E5%BC%80%E5%8F%91/" style="font-size: 15px;">开发</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 15px;">数据结构</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/07/06/Redis/">Redis</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/24/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/">Git笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/07/JVM/">JVM</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/27/%E7%88%AC%E8%99%AB/">爬虫</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/">计算机组成原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/Git%E7%AC%94%E8%AE%B0/">Git笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/manjaro%E9%85%8D%E7%BD%AE%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/">manjaro配置开发环境</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/docker%E7%AC%94%E8%AE%B0/">docker笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/13/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">dian的博客.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>