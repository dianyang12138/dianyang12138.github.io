<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="后端Java工程师"><title>Flink笔记 | dian的博客</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 4.2.1"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Flink笔记</h1><a id="logo" href="/.">dian的博客</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Flink笔记</h1><div class="post-meta">2019-11-26</div><div class="post-content"><a id="more"></a>

<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>Apache Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。</p>
<p>Flink比spark更快，延迟更低</p>
<p>高吞吐，低延迟，准确性高，良好的容错性</p>
<blockquote>
<p>有状态：指有些需要保存下来比对的数据</p>
</blockquote>
<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>数据报表、广告投放、实时报警、基站流量调配、实时结算和通知推送、实时监测异常行为</p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><p>事件驱动：程序依赖于事件的发生，来运转。与之相反的Spark流是按照时间来获取流数据</p>
<p>分层API：越顶层越抽象，表达、使用越简洁明了</p>
<p>越底层表达越丰富，越灵活</p>
<h5 id="流与批的世界观"><a href="#流与批的世界观" class="headerlink" title="流与批的世界观"></a>流与批的世界观</h5><p>在Flink内，所有都是流，离线数据是有界的流，实时数据是无界的流。</p>
<p>Spark内是批，离线数据是一个大批次，而实时数据是由一个一个无限的小批次组成的。</p>
<h5 id="其它特点"><a href="#其它特点" class="headerlink" title="其它特点"></a>其它特点</h5><ul>
<li>支持事件时间、处理时间</li>
<li>精确一次的状态一致性保证</li>
<li>低延迟，每秒处理百万级事件，毫秒级延迟</li>
<li>能与大多数存储系统对接</li>
<li>高可用，动态扩展，24小时运行</li>
</ul>
<h5 id="Flink和Spark的区别"><a href="#Flink和Spark的区别" class="headerlink" title="Flink和Spark的区别"></a>Flink和Spark的区别</h5><p>Spark：</p>
<ul>
<li>Spark的流实际是很小的批次，采用RDD模型</li>
<li>批计算，将DAG划分为不同的stage，一个完成后计算下一个</li>
</ul>
<p>Flink：</p>
<ul>
<li>Flink是数据流和事件序列</li>
<li>流执行模式，一个事件在一个节点处理完成后直接发往下一个节点处理</li>
</ul>
<h3 id="快速上手"><a href="#快速上手" class="headerlink" title="快速上手"></a>快速上手</h3><h4 id="wordcount："><a href="#wordcount：" class="headerlink" title="wordcount："></a>wordcount：</h4><p>pom文件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- hehttps://mvnrepository.com/artifact/org.apache.flink/flink-streaming-scala --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 该插件用于将Scala代码编译成class文件 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="comment">&lt;!-- 声明绑定到maven的compile阶段 --&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>引入Scala框架</p>
<h5 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//这里要引入org.apache.flink.api.scala下的所有，因为涉及到隐式转化</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @author dianyang</span></span><br><span class="line"><span class="comment"> * @date 2019/11/29 10:34</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建一个执行环境</span></span><br><span class="line">    <span class="keyword">val</span> environment: <span class="type">ExecutionEnvironment</span> = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//从文件中读取数据</span></span><br><span class="line">    <span class="keyword">val</span> inputPath: <span class="type">String</span> = <span class="string">"F:\\JAVAIDEA\\flink-learn\\src\\main\\resources\\hello.txt"</span></span><br><span class="line">    <span class="keyword">val</span> inputDateSet: <span class="type">DataSet</span>[<span class="type">String</span>] = environment.readTextFile(inputPath)</span><br><span class="line">    <span class="comment">//切分数据</span></span><br><span class="line">    <span class="keyword">val</span> wordCountDataSet: <span class="type">AggregateDataSet</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = inputDateSet.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).groupBy(<span class="number">0</span>).sum(<span class="number">1</span>)</span><br><span class="line">    wordCountDataSet.print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="流处理"><a href="#流处理" class="headerlink" title="流处理"></a>流处理</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//涉及到隐式转化，引入全部</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @author dianyang</span></span><br><span class="line"><span class="comment"> * @date 2019/11/29 16:03</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建流处理环境</span></span><br><span class="line">    <span class="keyword">val</span> environment: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//接受一个socket文本流</span></span><br><span class="line">    <span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">String</span>] = environment.socketTextStream(<span class="string">"localhost"</span>, <span class="number">7777</span>)</span><br><span class="line">    <span class="keyword">val</span> wordCountDataStream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = dataStream.flatMap(_.split(<span class="string">" "</span>)).filter(_.nonEmpty).map((_, <span class="number">1</span>)).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>)</span><br><span class="line">    wordCountDataStream.print()</span><br><span class="line">    <span class="comment">//真正开始执行</span></span><br><span class="line">    environment.execute(<span class="string">"冲冲冲"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Flink部署"><a href="#Flink部署" class="headerlink" title="Flink部署"></a>Flink部署</h2><h3 id="standalone模式"><a href="#standalone模式" class="headerlink" title="standalone模式"></a>standalone模式</h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>解压缩  flink-1.7.0-bin-hadoop27-scala_2.11.tgz</p>
<blockquote>
<p>支持的hadoop版本</p>
</blockquote>
<p><strong>修改 flink/conf/flink-conf.yaml 文件</strong></p>
<p>将<code>jobmanager.rpc.address: localhost</code>修改为<code>jobmanager.rpc.address: 集群master的ip</code></p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#flink/conf/flink-conf.yaml文件解析 </span></span><br><span class="line"><span class="comment">#可以运行的数量</span></span><br><span class="line"><span class="attr">taskmanager.numberOfTaskSlots:</span> <span class="number">1</span></span><br><span class="line"><span class="comment">#实际运行的数量</span></span><br><span class="line"><span class="attr">parallelism.default:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>jobmanager：管理调度</p>
<p>taskmanager：work，执行任务</p>
<p>slot：资源管理最小的单位</p>
<p><strong>修改 /conf/slave文件</strong></p>
<p>分行填入从机的ip地址，一个空格不能多一个不能少</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.1.1.1</span><br><span class="line">2.3.2.2</span><br></pre></td></tr></table></figure>

<p><code>bin/start-cluster.sh</code>群起</p>
<p>访问flink的web页面<code>http://Masterip:8081</code></p>
<h4 id="运行jar包"><a href="#运行jar包" class="headerlink" title="运行jar包"></a>运行jar包</h4><h5 id="图形界面"><a href="#图形界面" class="headerlink" title="图形界面"></a>图形界面</h5><p>将打包好的jar包通过页面的<code>Submit new job</code>-&gt;<code>add new</code>提交。</p>
<p>打勾选中，填写参数</p>
<ul>
<li>Entry class：填要运行的main函数的路径。就是导包的路径</li>
<li>Parallelism：并行度</li>
<li>Program Argumments：参数如<code>--host localhost --port 7777</code></li>
</ul>
<p>然后submit。</p>
<blockquote>
<p>flink的每个算子都可以设置并行度</p>
</blockquote>
<p>停止任务：右上角有Cancel按钮</p>
<h5 id="命令行下"><a href="#命令行下" class="headerlink" title="命令行下"></a>命令行下</h5><p>启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c 入口类 -p 并行度 jar包地址 --host localhost --port 7777</span><br></pre></td></tr></table></figure>

<p>查看</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink list</span><br></pre></td></tr></table></figure>

<blockquote>
<p>–all查看所有，包括已取消的</p>
</blockquote>
<p>停止</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/shell 用list查看到的运行的一串编号</span><br></pre></td></tr></table></figure>

<h3 id="yarn模式"><a href="#yarn模式" class="headerlink" title="yarn模式"></a>yarn模式</h3><ol>
<li><p>启动hadoop集群</p>
</li>
<li><p>启动yarn-session</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm test -d</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>-n(–container)</strong>：TaskManager的数量。</p>
<p><strong>-s(–slots)</strong>：   每个TaskManager的slot数量，默认一个slot一个core，默认每个taskmanager的slot的个数为1，有时可以多一些taskmanager，做冗余。</p>
<p><strong>-jm</strong>：JobManager的内存（单位MB)。</p>
<p><strong>-tm</strong>：每个taskmanager的内存（单位MB)。</p>
<p><strong>-nm</strong>：yarn 的appName(现在yarn的ui上的名字)。 </p>
<p><strong>-d</strong>：后台执行。</p>
</blockquote>
</li>
</ol>
<ol start="3">
<li><p>执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./flink run  -m yarn-cluster -c 入口类全路径  jar包  --input 输入路径 --output 输出路径</span><br></pre></td></tr></table></figure>
</li>
<li><p>去yarn控制台查看任务状态</p>
</li>
</ol>
<h3 id="K8S"><a href="#K8S" class="headerlink" title="K8S"></a>K8S</h3><p>也是一种重要的基于docker的部署方式。知道即可</p>
<h2 id="Flink运行组件和原理"><a href="#Flink运行组件和原理" class="headerlink" title="Flink运行组件和原理"></a>Flink运行组件和原理</h2><h3 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h3><p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/run_part.png" alt></p>
<p>重点：JobManager和TaskManager</p>
<ul>
<li>JobManager<ul>
<li>控制一个应用程序执行的主进程。就是说，每个应用程序都对应一个JobManager</li>
<li>JboManager会先接收要执行的应用程序。程序包括：作业图(JobGraph)、逻辑数据流图(logical dataflow graph)和打包了所有的类、库和其他资源的jar包</li>
<li>JobManager会将JobGraph转化为物理层面的数据流图(执行图”ExecutionGraph”)，包含了所有可以并发执行的任务</li>
<li>JobManager会向ResourceManager请求执行任务的资源，也就是TaskManager上的slot。一旦有足够的资源，就会方法到真正运行它们的TaskManager上。运行时，JobManager会负责所有需要中央协调的操作，如检查点(checkpoints)的保存记录</li>
</ul>
</li>
<li>TaskManager<ul>
<li>Flink中的工作进程。通常有多个TaskManager。每个都有一定量的slots。slots的数量限制了TaskManager能执行的任务数</li>
<li>启动之后，TaskManager会像resourceManager注册slots，收到resourceManager的指令，TaskManager会将slots供给给JobManager调用。JobManager就可以向slots分配任务执行</li>
<li>一个TaskeManager可以和其它运行同一应用程序的TaskManager交换数据</li>
</ul>
</li>
<li>ResourveManager<ul>
<li>主要负责TaskManager和slot</li>
<li>不同的环境和资源管理工具有不同的资源管理器。如：yarn、mesos、k8s，standlaone部署</li>
<li>当JM申请slot时，RM会将空闲的TM分配给JM。如果slot不够，会向资源提供平台发起会话，来启动TM进程的容器</li>
</ul>
</li>
<li>Dispatcher<ul>
<li>可以跨作业运行，为应用提交提供了REST接口</li>
<li>当一个应用被提交执行，分发器就会启动并将应用移交给一个JM</li>
<li>Dispatcher也会启动Web UI，来展示和监控作业执行的信息</li>
<li>Dispatcher在架构中不是必需的，取决于应用提交方式</li>
</ul>
</li>
</ul>
<h3 id="提交流程"><a href="#提交流程" class="headerlink" title="提交流程"></a>提交流程</h3><p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/running_img.png" alt></p>
<h4 id="yarn模式-1"><a href="#yarn模式-1" class="headerlink" title="yarn模式"></a>yarn模式</h4><p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/run_yarn.png" alt></p>
<p>这里的RM是yarn的RM</p>
<h4 id="调度原理"><a href="#调度原理" class="headerlink" title="调度原理"></a>调度原理</h4><p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/run_p.png" alt></p>
<p>Flink Program：我们写的代码通过转化，提交</p>
<h3 id="TaskManager和Slots"><a href="#TaskManager和Slots" class="headerlink" title="TaskManager和Slots"></a>TaskManager和Slots</h3><p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/TM_Sl.png" alt></p>
<ul>
<li><p>Flink中每个TaskManager都是一个JVM进程，它会在独立的线程上执行一个或多个subtask</p>
</li>
<li><p>为了控制一个TaskManager能接收多少task，TaskManager通过taskslot来进行控制（一个TaskManager至少一个slot）</p>
</li>
</ul>
<p>slot是在配置文件中提前定义好。或者是在创建yarnsession是指定。</p>
<p>一般按照每个TaskManager这个机器的性能，它所含有的资源来配置slots</p>
<p>slots中主要隔离的就是内存，cpu不做隔离</p>
<blockquote>
<p>一般内存是够用的，为了防止cpu复用，往往可以按照cpu数来划分</p>
</blockquote>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/TM_Sl_share.png" alt></p>
<ul>
<li>默认情况下，Flink允许子任务共享slot，即使它们是不同任务的子任务。这样的结果是，一个slot可以保存作业的整个管道</li>
<li>Task Slot是静态概念，是指TaskManager具有并发执行能力</li>
</ul>
<p>可以通过修改共享组不共享，默认是一个共享组</p>
<blockquote>
<p>这样不仅能防止资源倾斜，Flink只用计算最大任务的并行度即可，无需计算总和</p>
</blockquote>
<p>可以通过修改配置文件，运行时-p参数，代码内设置子任务并行度，代码内设置环境并行度来修改并行度</p>
<h2 id="数据流与执行图"><a href="#数据流与执行图" class="headerlink" title="数据流与执行图"></a>数据流与执行图</h2><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/DataFlow_2.png" alt></p>
<ul>
<li>所有Flink程序都是由三部分组成，Source、Transformation和Sink</li>
<li>Source负责读取数据源，Transformation利用各种算子进行处理加工，Sink负责输出</li>
</ul>
<h3 id="程序与数据流"><a href="#程序与数据流" class="headerlink" title="程序与数据流"></a>程序与数据流</h3><p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/DataFlow_1.png" alt></p>
<ul>
<li>运行时，Flink上运行的程序会被映射成”逻辑数据流”(dataflows)，它包含三部分</li>
<li>每个dataflow以一个或多个sources开始，以一个或多个sinks结束。dataflow类似于任意的有向无环图(DAG)</li>
<li>在大部分情况下，程序中的转换运算(transformations)和dataflow中的算子(operator)是一一对应的</li>
</ul>
<h4 id="执行图-ExecutionGraph"><a href="#执行图-ExecutionGraph" class="headerlink" title="执行图(ExecutionGraph)"></a>执行图(ExecutionGraph)</h4><p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/ExecutionGraph.png" alt></p>
<p>Flink中的执行图分为四层：StreamGraph-&gt;JobGraph-&gt;ExecutionGraph-&gt;物理执行图</p>
<ul>
<li>StreamGraph：是根据用户通过Stream API编写代码生成的最初的图。用来表示程序的拓扑结构。</li>
<li>JobGraph：StreamGraph经过优化生成的JobGraph，提交给JobManager的数据结构。主要的优化为，将多个符合条件的节点chain在一起作为一个节点</li>
<li>ExecutionGraph：JobManager根据JobGraph生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构</li>
<li>物理执行图：JobManager根据ExecutionGraph对Job进行调度后，在各个TaskManager上部署Task后形成的，并不是一个具体的数据结构</li>
</ul>
<blockquote>
<p>在客户端提交的时候就生成了JobGraph。</p>
<p>ExecutionGraph传给TaskManager就可以执行了</p>
</blockquote>
<h4 id="并行度（-Parallelism）"><a href="#并行度（-Parallelism）" class="headerlink" title="并行度（ Parallelism）"></a>并行度（ Parallelism）</h4><p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/Parallelism.png" alt></p>
<ul>
<li>一个特定算子的子任务(subtask)的个数被称之为其并行度(parallelism)。一般情况下，一个stream的并行度，可以认为就是其所有算子中最大的并行度</li>
</ul>
<blockquote>
<p>Flink里的并行非常的灵活</p>
</blockquote>
<p>这张图就体现了灵活的并行度</p>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/Parallelism_2.png" alt></p>
<blockquote>
<p>太多的并行可能会影响效率，这时候就要通过修改共享组来减少并行，提高效率</p>
</blockquote>
<p>一个程序中不同的算子可能有不同的并行度。所以算子之间传输数据的形式可以是one-to-one(forwarding)的模式也可以是redistributing的模式，具体取决于算子种类</p>
<ul>
<li>One-to-One：strem维护着分区以及元素顺序（比如source和map之间）。意味着map算子的子任务看到的元素的个数以及顺序根source算子的子任务生产的元素的个数、顺序相同。map、fliter、flatMap等算子都是改模式的对于酸洗。对应spark的窄依赖</li>
<li>Redistributing：stream的分区会发生改变。每一个算子的子任务一句所选择的transformation发送数据到不同的目标任务。例如，keyBy基于hashCode重分区、而broadcast和rebalance会随机重新分区，这些算子都会引起redistribute过程，而redistribute过程就类似于Spark中的shuffle。对应spark的宽依赖</li>
</ul>
<h4 id="任务链-Operator-Chains"><a href="#任务链-Operator-Chains" class="headerlink" title="任务链(Operator Chains)"></a>任务链(Operator Chains)</h4><ul>
<li>Flink采用了一种称为任务链的<strong>优化</strong>技术，可以在特定条件下减少本地通信的开销。为了满足任务链的要求，必须将俩个或多个算子设为相同的并行度，并通过本地转发（local forward）的方式进行连接</li>
<li>相同并行度的one-to-one操作，Flink这样相连的算子链接在一起形成一个task，原来的算子称为里面的subtask</li>
<li>并行度相同、并且是one-to-one操作，俩个条件缺一不可</li>
</ul>
<p>![](Flink笔记/Operator Chains.png)</p>
<p>在一些情况下我们并不希望很多的任务合并operator chain。有时候更希望它打散运行，让他有更好的并行度。</p>
<p>我们可以在代码内手动控制</p>
<p><code>env.disableOperatorChaining()</code>：在环境变量使用，是全部打散</p>
<p><code>.disableChaining()</code>：在算子后使用，后面全部打散</p>
<p><code>.startNewChain()</code>：开启一个新operatorChain。后面还会链接</p>
<h2 id="Flink流处理API"><a href="#Flink流处理API" class="headerlink" title="Flink流处理API"></a>Flink流处理API</h2><h3 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h3><p><code>ExecutionEnvironment.getExecutionEnvironment</code>：根据查询运行的方式决定返回什么样的运行环境，是最常用的一种创建执行环境的方式。</p>
<p><code>StreamExecutionEnvironment.createLocalEnvironment(1)</code>：返回本地执行环境，需要在调用时指定默认的并行度。</p>
<p><code>ExecutionEnvironment.createRemoteEnvironment(&quot;jobmanager-hostname&quot;, 6123,&quot;C://jar//flink//wordcount.jar&quot;)</code>：返回集群执行环境，将Jar提交到远程服务器。需要在调用时指定JobManager的IP和端口号，并指定要在集群中运行的Jar包。</p>
<h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><h4 id="简单source-用的少"><a href="#简单source-用的少" class="headerlink" title="简单source(用的少)"></a>简单source(用的少)</h4><p>有界流</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="comment">// 定义传感器数据样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SensorReading</span>(<span class="params">id: <span class="type">String</span>, timestamp: <span class="type">Long</span>, temperature: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">SourceTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 1. 从集合中读取数据</span></span><br><span class="line">    <span class="keyword">val</span> stream1 = env.fromCollection(<span class="type">List</span>(</span><br><span class="line">      <span class="type">SensorReading</span>(<span class="string">"sensor_1"</span>, <span class="number">1547718199</span>, <span class="number">35.80018327300259</span>),</span><br><span class="line">      <span class="type">SensorReading</span>(<span class="string">"sensor_6"</span>, <span class="number">1547718201</span>, <span class="number">15.402984393403084</span>),</span><br><span class="line">      <span class="type">SensorReading</span>(<span class="string">"sensor_7"</span>, <span class="number">1547718202</span>, <span class="number">6.720945201171228</span>),</span><br><span class="line">      <span class="type">SensorReading</span>(<span class="string">"sensor_10"</span>, <span class="number">1547718205</span>, <span class="number">38.101067604893444</span>)</span><br><span class="line">    ))</span><br><span class="line">    <span class="comment">//可以读取不同类型的元素，没有设置并行度时候默认为cpu核数</span></span><br><span class="line">    <span class="comment">//env.fromElements(1,2.1,"sad").print</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//读取文本文件</span></span><br><span class="line">    <span class="comment">//env.readTextFile("路径")</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在输出前会加上'stream1&gt; '并发情况下，会变成'stream1:1&gt; '的形式，&gt;前的数字为并发任务编号</span></span><br><span class="line">    stream1.print(<span class="string">"stream1"</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">    env.execute(<span class="string">"source api test"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Kafka-Source"><a href="#Kafka-Source" class="headerlink" title="Kafka Source"></a>Kafka Source</h4><p>pom</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--kafka的版本是0.11，scala的版本为2.11--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer011</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SourceTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 从kafka中读取数据</span></span><br><span class="line">    <span class="comment">// 创建kafka相关的配置</span></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">    properties.setProperty(<span class="string">"group.id"</span>, <span class="string">"consumer-group"</span>)</span><br><span class="line">    properties.setProperty(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>)</span><br><span class="line">    properties.setProperty(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>)</span><br><span class="line">    properties.setProperty(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>)</span><br><span class="line">    <span class="comment">//第一个参数：topic，第二个参数：值的反序列化工具，第三个参数：配置参数</span></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>](<span class="string">"sensor"</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br><span class="line">    stream.print(<span class="string">"stream"</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//bin/kafka-console-producer.sh --broker-list localhost:9092 --topic sensor启动kafka生产者</span></span><br><span class="line">    env.execute(<span class="string">"source api test"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>容灾机制：如果kafka已经发送了数据，我们就回复接收。如果在数据转化时数据丢失就无法回滚，所以Spark选择在数据接收转化后再回复。</p>
<p>Flink是有状态的流处理，可以把kafka的偏移量作为状态保存。如果数据丢失，Flink会自动修改偏移量</p>
</blockquote>
<h4 id="自定义Source"><a href="#自定义Source" class="headerlink" title="自定义Source"></a>自定义Source</h4><p>测试环境中，可能需要自己自定义Source来生成测试数据，但是生产环境中很少</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer011</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SensorReading</span>(<span class="params"> id: <span class="type">String</span>, timestamp: <span class="type">Long</span>, temperature: <span class="type">Double</span> </span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//基础SourceFunction</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">SensorSource</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">SourceFunction</span>[<span class="type">SensorReading</span>]</span>&#123;</span><br><span class="line">    <span class="comment">// 定义一个flag：表示数据源是否还在正常运行</span></span><br><span class="line">    <span class="keyword">var</span> running: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = running = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">SensorReading</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 创建一个随机数发生器</span></span><br><span class="line">        <span class="keyword">val</span> rand = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line">        <span class="comment">// 随机初始换生成10个传感器的温度数据，之后在它基础随机波动生成流数据</span></span><br><span class="line">        <span class="keyword">var</span> curTemp = <span class="number">1.</span>to(<span class="number">10</span>).map(</span><br><span class="line">            i =&gt; ( <span class="string">"sensor_"</span> + i, <span class="number">60</span> + rand.nextGaussian() * <span class="number">20</span> )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 无限循环生成流数据，除非被cancel</span></span><br><span class="line">        <span class="keyword">while</span>(running)&#123;</span><br><span class="line">            <span class="comment">// 更新温度值</span></span><br><span class="line">            curTemp = curTemp.map(</span><br><span class="line">                t =&gt; (t._1, t._2 + rand.nextGaussian())</span><br><span class="line">            )</span><br><span class="line">            <span class="comment">// 获取当前的时间戳</span></span><br><span class="line">            <span class="keyword">val</span> curTime = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">            <span class="comment">// 包装成SensorReading，输出</span></span><br><span class="line">            curTemp.foreach(</span><br><span class="line">                t =&gt; ctx.collect( <span class="type">SensorReading</span>(t._1, curTime, t._2) )</span><br><span class="line">            )</span><br><span class="line">            <span class="comment">// 间隔100ms</span></span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">100</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h3><h4 id="基本转化算子"><a href="#基本转化算子" class="headerlink" title="基本转化算子"></a>基本转化算子</h4><ul>
<li>map：<code>val streamMap = stream.map { x =&gt; x * 2 }</code></li>
<li>flatmap：传数组进入会打散。<code>val streamFlatMap = stream.flatMap{ x =&gt; x.split(&quot; &quot;)}</code></li>
<li>filter：过滤，返回bool类型 <code>val streamFilter = stream.filter{x =&gt; x == 1}</code></li>
</ul>
<h4 id="分组转化算子-键控流"><a href="#分组转化算子-键控流" class="headerlink" title="分组转化算子(键控流)"></a>分组转化算子(键控流)</h4><ul>
<li>KeyBy：从dataStream=&gt;KeyedStream：流还是单个流，内部做了分区。每个分区有相同的key，内部以hash实现</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//传入int，Int为传入元素的键值位置 返回</span></span><br><span class="line"><span class="comment">//T为dataStream的泛型</span></span><br><span class="line"><span class="comment">//JavaTuple就是KeyedStream[T, K]的K</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">keyBy</span></span>(fields: <span class="type">Int</span>*):<span class="type">KeyedStream</span>[<span class="type">T</span>, <span class="type">JavaTuple</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">//传入函数</span></span><br><span class="line"><span class="comment">//T为dataStream的泛型</span></span><br><span class="line"><span class="comment">//K为函数返回值的类型 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">keyBy</span></span>[<span class="type">K</span>: <span class="type">TypeInformation</span>](fun: <span class="type">T</span> =&gt; <span class="type">K</span>): <span class="type">KeyedStream</span>[<span class="type">T</span>, <span class="type">K</span>]</span><br></pre></td></tr></table></figure>

<h4 id="滚动聚合算子-Rolling-Aggregation"><a href="#滚动聚合算子-Rolling-Aggregation" class="headerlink" title="滚动聚合算子(Rolling Aggregation)"></a>滚动聚合算子(Rolling Aggregation)</h4><p>dataStream没有这些算子。这些算子针能对keyedStream的每一支流做聚合。会返回<code>DataStream</code>。</p>
<ul>
<li>sum()：可以传位置参数，也可以传属性名</li>
<li>min()</li>
<li>max()</li>
<li>minBy()</li>
<li>maxBy()</li>
</ul>
<blockquote>
<p>如果数据很简单，需要直接使用聚合算子，可以先造一个无用的key，然后keyby转化再用聚合算子</p>
</blockquote>
<ul>
<li>Reduce</li>
</ul>
<p>KeyedStream=&gt;DataStream</p>
<p>一个分组数据流的聚合操作，合并当前的元素和上次聚合的结果，产生一个新的值，返回的流中<strong>包含每一次聚合的结果</strong>，而不是只返回最后一次聚合的最终结果。</p>
<p>Reduce：传入一个需要俩个参数的函数，第一个参数为之前聚合的值，第二个为新传入的值</p>
<h4 id="多流转化算子"><a href="#多流转化算子" class="headerlink" title="多流转化算子"></a>多流转化算子</h4><ul>
<li>split和 Select</li>
</ul>
<p><code>Split</code>：将strean转化为SplitStream，SplitStream里面有俩个组。但是无法对里面的流进行操作。必须用select摘取出来，才能操作。</p>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/spilt.png" alt></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//分流，根据温度是否大于30度划分</span></span><br><span class="line"><span class="keyword">val</span> splitStream = dataStream</span><br><span class="line">  .split(sensorData =&gt; &#123;<span class="comment">//seq是TraversableOnce的实现类，返回一个戳标记一下</span></span><br><span class="line">    <span class="keyword">if</span> (sensorData.temperature &gt; <span class="number">30</span>) <span class="type">Seq</span>(<span class="string">"high"</span>) <span class="keyword">else</span> <span class="type">Seq</span>(<span class="string">"low"</span>)</span><br><span class="line">  &#125;)</span><br><span class="line"><span class="comment">//用select选出来</span></span><br><span class="line"><span class="keyword">val</span> highTempStream = splitStream.select(<span class="string">"high"</span>)</span><br><span class="line"><span class="keyword">val</span> lowTempStream = splitStream.select(<span class="string">"low"</span>)</span><br><span class="line"><span class="keyword">val</span> allTempStream = splitStream.select(<span class="string">"high"</span>, <span class="string">"low"</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>Connect</li>
</ul>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/Connect.png" alt></p>
<p>两个流被放在了一个同一个流中，内部依然保持各自的数据和形式不发生任何变化，两个流相互独立。</p>
<ul>
<li>CoMap</li>
</ul>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/CoMap.png" alt></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> warningStream = highTempStream.map(sensorData =&gt; (sensorData.id, sensorData.temperature))</span><br><span class="line"><span class="keyword">val</span> connectedStreams = warningStream.connect(lowTempStream)</span><br><span class="line"><span class="keyword">val</span> coMapStream: <span class="type">DataStream</span>[<span class="type">Product</span>] = connectedStreams.map(</span><br><span class="line">  warningData =&gt; (warningData._1, warningData._2, <span class="string">"high temperature warning"</span>),</span><br><span class="line">  lowData =&gt; (lowData.id, <span class="string">"healthy"</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<ul>
<li>Union</li>
</ul>
<p><code>val unionStream = highTempStream.union(lowTempStream)</code></p>
<p>两个以上的DataStream进行union 产生一个包含所有DataStream元素的新DataStream。DataStream可以跟它自己做union操作，在新的DataStream中，你将看到每一个元素都出现两次。</p>
<blockquote>
<p>Union之前两个流的类型必须是一样，Connect可以不一样，在之后的coMap中再去调整成为一样的<br>Connect只能操作两个流，Union可以操作多个</p>
</blockquote>
<h3 id="支持的数据类型"><a href="#支持的数据类型" class="headerlink" title="支持的数据类型"></a>支持的数据类型</h3><ul>
<li><p>基础数据类型：所有的Java和Scala基础数据类型如Int、Doublt…</p>
</li>
<li><p>Java和Scala元组(Tuples)</p>
</li>
<li><p>Scala样例类</p>
</li>
<li><p>Java的简单对象(POJOs)</p>
</li>
<li><p>其他(Arrays，list，maps，enums等等)</p>
</li>
</ul>
<h3 id="实现UDF函数"><a href="#实现UDF函数" class="headerlink" title="实现UDF函数"></a>实现UDF函数</h3><p>可以实现更细粒度的控制流</p>
<h4 id="函数类"><a href="#函数类" class="headerlink" title="函数类"></a>函数类</h4><p>写一个类继承函数对象。重写内部方法。然后传实例化该类的对象。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用</span></span><br><span class="line">dataStream.filter( <span class="keyword">new</span> <span class="type">MyFilter</span>() ).print()</span><br><span class="line"><span class="comment">//定义类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyFilter</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">FilterFunction</span>[<span class="type">SensorReading</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">SensorReading</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    value.id.startsWith(<span class="string">"sensor_1"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">RichMapFunction</span>[<span class="type">SensorReading</span>, <span class="type">String</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: <span class="type">SensorReading</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="string">"flink"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = <span class="keyword">super</span>.open(parameters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>函数类，可以在new的时候，传额外参数</p>
</blockquote>
<h4 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h4><p>传入函数，即是我们一般用的</p>
<h4 id="富函数"><a href="#富函数" class="headerlink" title="富函数"></a>富函数</h4><p>“富函数”是Flink提供的函数类接口。它可以获取运行环境上下文，有生命周期方法，能实现更复杂的功能。每个函数都有对应的接口。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//第一个泛型是输入，第二个泛型是输出</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyFlatMap</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>[<span class="type">Int</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="comment">//做初始化内容等等</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//具体工作</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(in: <span class="type">Int</span>, collector: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    collector.collect(<span class="string">"sad"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//关闭工作</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h3><p>Flink中没有类似Spark中的行动算子。所有的对外输出操作都由Sink来做。最后都通过类似下面的方式完成<code>stream.addSink(new XXSink(xxx))</code>。我们常用的print方法返回的也是Sink</p>
<p>官方提供了一部分的框架的sink。除此之外的，需要自定义</p>
<ul>
<li>Kafka</li>
<li>Cassandra</li>
<li>Amazon Kinesis Streams</li>
<li>Elasticsearch</li>
<li>Hadoop FileSystem</li>
<li>RabbitMQ</li>
<li>Apache NIFI</li>
<li>Twitter Streaming Api</li>
<li>Bahir提供支持的<ul>
<li>ActiveMQ</li>
<li>Flume</li>
<li>Redis</li>
<li>Akka</li>
<li>Netty</li>
</ul>
</li>
</ul>
<h4 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h4><p>pom</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.atguigu.apitest.<span class="type">SensorReading</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaProducer011</span>.<span class="type">Semantic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.&#123;<span class="type">FlinkKafkaConsumer011</span>, <span class="type">FlinkKafkaProducer011</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaSinkTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">        env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// source 文件读取</span></span><br><span class="line">        <span class="comment">//    val inputStream = env.readTextFile("D:\\Projects\\BigData\\FlinkTutorial\\src\\main\\resources\\sensor.txt")</span></span><br><span class="line">        <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">        properties.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">        properties.setProperty(<span class="string">"group.id"</span>, <span class="string">"consumer-group"</span>)</span><br><span class="line">        properties.setProperty(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>)</span><br><span class="line">        properties.setProperty(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>)</span><br><span class="line">        properties.setProperty(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>)</span><br><span class="line">        <span class="comment">//kafka读取</span></span><br><span class="line">        <span class="keyword">val</span> inputStream = env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>](<span class="string">"sensor"</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Transform操作</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> dataStream = inputStream</span><br><span class="line">        .map(</span><br><span class="line">            data =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">                <span class="type">SensorReading</span>( dataArray(<span class="number">0</span>).trim, dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble ).toString  <span class="comment">// 转成String方便序列化输出</span></span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">/*new FlinkKafkaProducer011[(String)]("localhost", "topic", new SimpleStringSchema())*/</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// sink </span></span><br><span class="line">        dataStream.addSink( <span class="keyword">new</span> <span class="type">FlinkKafkaProducer011</span>[<span class="type">String</span>]( <span class="string">"sinkTest"</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties) )</span><br><span class="line">        dataStream.print()</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"kafka sink test"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>后面会讲端到端状态一致性。</p>
</blockquote>
<h4 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h4><p>pom</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.<span class="type">RedisSink</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.config.<span class="type">FlinkJedisPoolConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.&#123;<span class="type">RedisCommand</span>, <span class="type">RedisCommandDescription</span>, <span class="type">RedisMapper</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RedisSinkTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// source</span></span><br><span class="line">    <span class="keyword">val</span> inputStream = env.readTextFile(<span class="string">"F:\\JAVAIDEA\\flink-learn\\src\\main\\resources\\sensor.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// transform</span></span><br><span class="line">    <span class="keyword">val</span> dataStream = inputStream</span><br><span class="line">      .map(</span><br><span class="line">        data =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">          <span class="type">SensorReading</span>( dataArray(<span class="number">0</span>).trim, dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble )</span><br><span class="line">        &#125;</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">FlinkJedisPoolConfig</span>.<span class="type">Builder</span>()</span><br><span class="line">      .setHost(<span class="string">"localhost"</span>)</span><br><span class="line">      .setPort(<span class="number">6379</span>)</span><br><span class="line">      .build()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// sink</span></span><br><span class="line">    dataStream.addSink( <span class="keyword">new</span> <span class="type">RedisSink</span>(conf, <span class="keyword">new</span> <span class="type">MyRedisMapper</span>()) )</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"redis sink test"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRedisMapper</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">RedisMapper</span>[<span class="type">SensorReading</span>]</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义保存数据到redis的命令</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCommandDescription</span></span>: <span class="type">RedisCommandDescription</span> = &#123;</span><br><span class="line">    <span class="comment">// 把传感器id和温度值保存成哈希表 HSET key field value</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">RedisCommandDescription</span>( <span class="type">RedisCommand</span>.<span class="type">HSET</span>, <span class="string">"sensor_temperature"</span> )</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义保存到redis的value</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValueFromData</span></span>(t: <span class="type">SensorReading</span>): <span class="type">String</span> = t.temperature.toString</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义保存到redis的key</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getKeyFromData</span></span>(t: <span class="type">SensorReading</span>): <span class="type">String</span> = t.id</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="ES"><a href="#ES" class="headerlink" title="ES"></a>ES</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.httpcomponents<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>httpclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.5.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RuntimeContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.&#123;<span class="type">ElasticsearchSinkFunction</span>, <span class="type">RequestIndexer</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch6.<span class="type">ElasticsearchSink</span></span><br><span class="line"><span class="keyword">import</span> org.apache.http.<span class="type">HttpHost</span></span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.<span class="type">Requests</span></span><br><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">EsSinkTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// source</span></span><br><span class="line">    <span class="keyword">val</span> inputStream = env.readTextFile(<span class="string">"D:\\Projects\\BigData\\FlinkTutorial\\src\\main\\resources\\sensor.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// transform</span></span><br><span class="line">    <span class="keyword">val</span> dataStream = inputStream</span><br><span class="line">      .map(</span><br><span class="line">        data =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">          <span class="type">SensorReading</span>( dataArray(<span class="number">0</span>).trim, dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble )</span><br><span class="line">        &#125;</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> httpHosts = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">HttpHost</span>]()</span><br><span class="line">    httpHosts.add(<span class="keyword">new</span> <span class="type">HttpHost</span>(<span class="string">"localhost"</span>, <span class="number">9200</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个esSink 的builder</span></span><br><span class="line">    <span class="keyword">val</span> esSinkBuilder = <span class="keyword">new</span> <span class="type">ElasticsearchSink</span>.<span class="type">Builder</span>[<span class="type">SensorReading</span>](</span><br><span class="line">      httpHosts,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ElasticsearchSinkFunction</span>[<span class="type">SensorReading</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(element: <span class="type">SensorReading</span>, ctx: <span class="type">RuntimeContext</span>, indexer: <span class="type">RequestIndexer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">          println(<span class="string">"saving data: "</span> + element)</span><br><span class="line">          <span class="comment">// 包装成一个Map或者JsonObject</span></span><br><span class="line">          <span class="keyword">val</span> json = <span class="keyword">new</span> util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</span><br><span class="line">          json.put(<span class="string">"sensor_id"</span>, element.id)</span><br><span class="line">          json.put(<span class="string">"temperature"</span>, element.temperature.toString)</span><br><span class="line">          json.put(<span class="string">"ts"</span>, element.timestamp.toString)</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 创建index request，准备发送数据</span></span><br><span class="line">          <span class="keyword">val</span> indexRequest = <span class="type">Requests</span>.indexRequest()</span><br><span class="line">            .index(<span class="string">"sensor"</span>)</span><br><span class="line">            .`<span class="class"><span class="keyword">type</span>`(<span class="params">"readingdata"</span>)</span></span><br><span class="line"><span class="class">            .<span class="title">source</span>(<span class="params">json</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">          <span class="title">//</span> <span class="title">利用index发送请求，写入数据</span></span></span><br><span class="line"><span class="class">          <span class="title">indexer</span>.<span class="title">add</span>(<span class="params">indexRequest</span>)</span></span><br><span class="line"><span class="class">          <span class="title">println</span>(<span class="params">"data saved."</span>)</span></span><br><span class="line"><span class="class">        &#125;</span></span><br><span class="line"><span class="class">      &#125;</span></span><br><span class="line"><span class="class">    )</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">//</span> <span class="title">sink</span></span></span><br><span class="line"><span class="class">    <span class="title">dataStream</span>.<span class="title">addSink</span>(<span class="params"> esSinkBuilder.build(</span>) )</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">env</span>.<span class="title">execute</span>(<span class="params">"es sink test"</span>)</span></span><br><span class="line"><span class="class">  &#125;</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>

<h4 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.44<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.&#123;<span class="type">Connection</span>, <span class="type">DriverManager</span>, <span class="type">PreparedStatement</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.&#123;<span class="type">RichSinkFunction</span>, <span class="type">SinkFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JdbcSinkTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// source</span></span><br><span class="line">    <span class="keyword">val</span> inputStream = env.readTextFile(<span class="string">"D:\\Projects\\BigData\\FlinkTutorial\\src\\main\\resources\\sensor.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// transform</span></span><br><span class="line">    <span class="keyword">val</span> dataStream = inputStream</span><br><span class="line">      .map(</span><br><span class="line">        data =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">          <span class="type">SensorReading</span>(dataArray(<span class="number">0</span>).trim, dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble)</span><br><span class="line">        &#125;</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// sink</span></span><br><span class="line">    dataStream.addSink(<span class="keyword">new</span> <span class="type">MyJdbcSink</span>())</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"jdbc sink test"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyJdbcSink</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>[<span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 定义sql连接、预编译器</span></span><br><span class="line">  <span class="keyword">var</span> conn: <span class="type">Connection</span> = _</span><br><span class="line">  <span class="keyword">var</span> insertStmt: <span class="type">PreparedStatement</span> = _</span><br><span class="line">  <span class="keyword">var</span> updateStmt: <span class="type">PreparedStatement</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 初始化，创建连接和预编译语句</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">super</span>.open(parameters)</span><br><span class="line">    conn = <span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test"</span>, <span class="string">"root"</span>, <span class="string">"123456"</span>)</span><br><span class="line">    insertStmt = conn.prepareStatement(<span class="string">"INSERT INTO temperatures (sensor, temp) VALUES (?,?)"</span>)</span><br><span class="line">    updateStmt = conn.prepareStatement(<span class="string">"UPDATE temperatures SET temp = ? WHERE sensor = ?"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 调用连接，执行sql</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(value: <span class="type">SensorReading</span>, context: <span class="type">SinkFunction</span>.<span class="type">Context</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 执行更新语句</span></span><br><span class="line">    updateStmt.setDouble(<span class="number">1</span>, value.temperature)</span><br><span class="line">    updateStmt.setString(<span class="number">2</span>, value.id)</span><br><span class="line">    updateStmt.execute()</span><br><span class="line">    <span class="comment">// 如果update没有查到数据，那么执行插入语句</span></span><br><span class="line">    <span class="keyword">if</span> (updateStmt.getUpdateCount == <span class="number">0</span>) &#123;</span><br><span class="line">      insertStmt.setString(<span class="number">1</span>, value.id)</span><br><span class="line">      insertStmt.setDouble(<span class="number">2</span>, value.temperature)</span><br><span class="line">      insertStmt.execute()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 关闭时做清理工作</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    insertStmt.close()</span><br><span class="line">    updateStmt.close()</span><br><span class="line">    conn.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Window-API"><a href="#Window-API" class="headerlink" title="Window API"></a>Window API</h2><h3 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>窗口API。流是无界的，窗口时有界的。哦那个窗口截取流。获得有界流。</p>
<p>窗口就是将无限流切割为有限流的一种方式，它会将流数据分发到有限大小的桶(bucket)当中</p>
<h4 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h4><ul>
<li><p>时间窗口</p>
<ul>
<li>滚动时间窗口</li>
<li>滑动时间窗口</li>
<li>会话窗口</li>
</ul>
</li>
<li><p>计数窗口</p>
<ul>
<li>滚动计数窗口</li>
<li>滑动计数窗口</li>
</ul>
</li>
</ul>
<p>滚动窗口：依据窗口长度对数据进行切分。没有重叠，前后对齐</p>
<p>滑动窗口：长度固定，有重叠部分</p>
<p>会话窗口：由一系列事件组合一个指定时间长度的timeout间隙组成，类似于web应用的session，也就是一段时间没有接收到新数据就会生成新的窗口。</p>
<blockquote>
<p>会话窗口，就是数据流如果超过一段时间，就会隔断</p>
</blockquote>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/session_window.png" alt></p>
<h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><p>窗口分配器——window()方法</p>
<ul>
<li>window方法只能用于keyedStream。</li>
<li>Flink提供了简捷的timewindow和countwindow方法，用于定义时间窗口和计数窗口</li>
</ul>
<h4 id="窗口分配器"><a href="#窗口分配器" class="headerlink" title="窗口分配器"></a>窗口分配器</h4><ul>
<li><p>window方法接收一个WindowAssigner</p>
</li>
<li><p>WindowAssigner负责将每条输入的数据分发到window中</p>
</li>
<li><p>Flink提供了通用的WindowAssigner</p>
<ul>
<li>滚动窗口</li>
<li>滑动窗口</li>
<li>会话窗口</li>
<li>全局窗口：所有数据放一个窗口</li>
</ul>
<blockquote>
<p>全局窗口一般是自定义的时候使用。</p>
</blockquote>
</li>
</ul>
<h4 id="创建窗口"><a href="#创建窗口" class="headerlink" title="创建窗口"></a>创建窗口</h4><ul>
<li><p>滚动时间窗口：<code>.timeWindow(Time.seconds(15))</code></p>
</li>
<li><p>滑动时间窗口：<code>.timeWindow(Time.seconds(15),Time.seconds(5))</code></p>
</li>
<li><p>会话窗口：<code>window(EventTimeSessionWindows.withGap(Time.minutes(10)))</code></p>
</li>
<li><p>滚动计数窗口：<code>countWindow(5)</code></p>
</li>
<li><p>滑动计数窗口：<code>.countWindow(10,2)</code></p>
</li>
</ul>
<h4 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h4><p>窗口函数定义了对窗口内的数据做的操作</p>
<ul>
<li><p>增量聚合函数</p>
<ul>
<li>每条数据到来就进行计算，保持一个简单的状态</li>
<li>ReduceFunction,AggregateFunction(聚合函数)</li>
</ul>
<blockquote>
<p>增量就是，在窗口内像流一样，来一条收一条</p>
</blockquote>
</li>
<li><p>全窗口函数</p>
<ul>
<li>先把窗口所有数据收集起来，等到计算的时候会遍历所有数据。能做的事情相较于增量聚合更多</li>
<li>ProcessWindowFunction：会回到dataStream</li>
</ul>
</li>
</ul>
<h4 id="其他可选API"><a href="#其他可选API" class="headerlink" title="其他可选API"></a>其他可选API</h4><ul>
<li>.trigger()——触发器：定义window什么时候关闭，触发计算并输出结果</li>
<li>.evitor()——移除器：定义移除某些数据的逻辑，可选移除时间</li>
<li>.allowedLatenss()——允许处理迟到的数据：窗口关闭后，延迟的数据。默认不允许</li>
<li>.sideOutputLateData()——将迟到的数据放入侧输出流：</li>
<li>.getSideOutput()——获得侧输出流</li>
</ul>
<h3 id="时间语义和watermark"><a href="#时间语义和watermark" class="headerlink" title="时间语义和watermark"></a>时间语义和watermark</h3><h4 id="时间语义"><a href="#时间语义" class="headerlink" title="时间语义"></a>时间语义</h4><p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/time.png" alt></p>
<ul>
<li>Event Time：事件创建的时间</li>
<li>Ingestion Time：数据进入Flink的事件</li>
<li>Processing Time：执行操作算子的本地系统事件，与机器相关</li>
</ul>
<h4 id="在代码中设置Event-TIme"><a href="#在代码中设置Event-TIme" class="headerlink" title="在代码中设置Event TIme"></a>在代码中设置Event TIme</h4><p>在代码中，对执行环境调用<code>.setStreamTimeCharacteristic</code>方法，设置流时间特性，具体的时间需要从数据中提取时间戳，Flink无法自动提取</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="comment">//从调用时刻开始给env创建的每个stream追加时间特征</span></span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EvenTime</span>)</span><br></pre></td></tr></table></figure>

<h4 id="水位线-watermark"><a href="#水位线-watermark" class="headerlink" title="水位线(watermark)"></a>水位线(watermark)</h4><p>时一直衡量Event Time进展的机制，可以设置延迟触发。</p>
<p>watermark用来处理乱序时间，一般解和window实现。</p>
<p>数据流中watermark用于表示timestamp小雨water mark的数据都已经到达。</p>
<p>所以window的执行由watermark触发</p>
<p>watermark用来让程序自己平衡延迟和结果准确性</p>
<h5 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h5><ul>
<li>watermark是一条特殊的数据记录。</li>
<li>watermark必须单调递增，来确保任务的事件时间时钟在向前推进</li>
<li>watermark与数据的时间戳相关</li>
</ul>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/watermark.png" alt></p>
<p>每个时间窗口，内部都有事件时间的时钟。由watermark推进时钟</p>
<p>一个分区对应一个分区watermark</p>
<p>task的watermark以最小的分区watermark为准</p>
<p>向下游广播已接受的有序的数据</p>
<h5 id="watermark的引入"><a href="#watermark的引入" class="headerlink" title="watermark的引入"></a>watermark的引入</h5><p>指定数据源中的时间戳和延时</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dataStream.assignTimestampsAndWatermarks(<span class="comment">//数据流的泛型</span></span><br><span class="line">  <span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">String</span>]</span><br><span class="line">  <span class="comment">//延迟时间</span></span><br><span class="line">    (<span class="type">Time</span>.milliseconds(<span class="number">1000</span>)) &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(t: <span class="type">String</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">        <span class="comment">//指定数据流的时间戳</span></span><br><span class="line">      t.toInt*<span class="number">100</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>只指定时间戳<code>dataStream.assignAscendingTimestamps(_.timestamp*1000)</code></p>
<p>也可以自定义类传入<code>dataStream.assignTimestampsAndWatermarks(new MyAssigner())</code></p>
<p>Flink提供了俩种类型，这俩种都继承TimestampAssigner</p>
<ul>
<li>AssignerWithPeriodicWatermarks<ul>
<li>周期性生产watermark；系统会周期性将watermark插入流中（Processing Time）</li>
<li>默认周期200mm，可以使用ExecutionConfig.setAutoWatermarkInterval()进行设置</li>
<li>升序和前面乱序的处理BoundedOutOfOrderness，都是基于周期性watermark的</li>
</ul>
</li>
<li>AssignerWithPunctuatedWatermarks<ul>
<li>没有时间周期。可以打断生成</li>
</ul>
</li>
</ul>
<h4 id="watermark设定"><a href="#watermark设定" class="headerlink" title="watermark设定"></a>watermark设定</h4><p>watermark由应用程序开发人员生成</p>
<p>延迟设久了，收到结果的速度就很慢，解决办法就是在水位线到达之前输出近似值</p>
<p>watermark设短了，看你会收到错误结果，就要设置延迟处理</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//周期性生成</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PeriodicAssigner</span> <span class="keyword">extends</span> <span class="title">AssignerWithPeriodicWatermarks</span>[<span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> bound: <span class="type">Long</span> = <span class="number">60</span> * <span class="number">1000</span></span><br><span class="line">  <span class="keyword">var</span> maxTs: <span class="type">Long</span> = <span class="type">Long</span>.<span class="type">MinValue</span></span><br><span class="line">  <span class="comment">//周个周期会获取这个方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCurrentWatermark</span></span>: <span class="type">Watermark</span> = &#123;</span><br><span class="line">    <span class="comment">//当前最大时间戳减去延迟</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Watermark</span>(maxTs - bound)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//抽取时间戳</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(t: <span class="type">SensorReading</span>, l: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">    maxTs = maxTs.max(t.timestamp)</span><br><span class="line">    t.timestamp</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//断点</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PunctuatedAssigner</span> <span class="keyword">extends</span> <span class="title">AssignerWithPunctuatedWatermarks</span>[<span class="type">SensorReading</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">val</span> bound:<span class="type">Long</span>=<span class="number">60</span>*<span class="number">1000</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">checkAndGetNextWatermark</span></span>(t: <span class="type">SensorReading</span>, l: <span class="type">Long</span>): <span class="type">Watermark</span> = &#123;</span><br><span class="line">    <span class="comment">//可以通过指定条件触发watermanrk</span></span><br><span class="line">    <span class="keyword">if</span>(t.id==<span class="string">"sensor_1"</span>)&#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Watermark</span>(l-bound)</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">      <span class="literal">null</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(t: <span class="type">SensorReading</span>, l: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">    t.timestamp</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>没设置时间戳默认为Processing Time</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Processing Time</span></span><br><span class="line"><span class="comment">//周期10秒，选出10秒内最小的</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 设置事件时间</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> inputStream = env.socketTextStream(<span class="string">"192.168.5.102"</span>, <span class="number">7777</span>)</span><br><span class="line">    <span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = inputStream.map(data =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">      <span class="comment">//case class SensorReading( id: String, timestamp: Long, temperature: Double )</span></span><br><span class="line">      <span class="type">SensorReading</span>(dataArray(<span class="number">0</span>).trim, dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble)</span><br><span class="line">    &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//dataStream.map(data =&gt; (data.id, data.timestamp)).print</span></span><br><span class="line">    <span class="keyword">val</span> minTimeStream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Long</span>)] =</span><br><span class="line">    dataStream.map(data =&gt; (data.id, data.timestamp))</span><br><span class="line">      .keyBy(_._1).timeWindow(<span class="type">Time</span>.seconds(<span class="number">10</span>))</span><br><span class="line">      .reduce((data1, data2) =&gt; (data1._1, math.min(data2._2, data1._2)))</span><br><span class="line"></span><br><span class="line">    minTimeStream.print()</span><br><span class="line">    env.execute(<span class="string">"window api test"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>timeWindow(Time.seconds(10), Time.seconds(3))</code>意思是滑动窗口15秒大，每3秒滑动一次</p>
<p>窗口：包含开始没包含结束</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Event time</span></span><br><span class="line"><span class="comment">//滑动窗口大小10，每4秒滑动一次</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 设置事件时间</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置周期性时间</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">300</span>)</span><br><span class="line">    <span class="comment">// 设置事件时间</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> inputStream = env.socketTextStream(<span class="string">"192.168.5.102"</span>, <span class="number">7777</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> valueStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = inputStream.map(data =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">      <span class="comment">//case class SensorReading( id: String, timestamp: Long, temperature: Double )</span></span><br><span class="line">      <span class="type">SensorReading</span>(dataArray(<span class="number">0</span>).trim, dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble)</span><br><span class="line">    &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = valueStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">SensorReading</span>](<span class="type">Time</span>.seconds(<span class="number">1</span>)) &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(t: <span class="type">SensorReading</span>): <span class="type">Long</span> = t.timestamp * <span class="number">1000</span>L</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//dataStream.map(data =&gt; (data.id, data.timestamp)).print</span></span><br><span class="line">    <span class="keyword">val</span> minTimeStream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Long</span>)] =</span><br><span class="line">      dataStream.map(data =&gt; (data.id, data.timestamp))</span><br><span class="line">        .keyBy(_._1).timeWindow(<span class="type">Time</span>.seconds(<span class="number">10</span>), <span class="type">Time</span>.seconds(<span class="number">4</span>))</span><br><span class="line">        .reduce((data1, data2) =&gt; (data1._1, math.min(data2._2, data1._2)))</span><br><span class="line"></span><br><span class="line">    minTimeStream.print()</span><br><span class="line">    env.execute(<span class="string">"window api test"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>.timeWindow(SlidingEventTimeWindows.of(Time.seconds(15),Time.seconds(5),Time.hours(-8)))</code>可以通过该方法指定时间偏移量（时差）</p>
<h2 id="ProcessFunction-API-底层"><a href="#ProcessFunction-API-底层" class="headerlink" title="ProcessFunction API(底层)"></a>ProcessFunction API(底层)</h2><p>先前的转化算子无法访问事件的时间戳信息和水位线信息。</p>
<p>ProcessFunction API可以访问时间戳、watermark以及注册定时事件。还能输出特定一些事件。</p>
<p>共8个ProcessFunction：</p>
<ul>
<li>ProcessFunction</li>
<li>KeyedProcessFunction</li>
<li>CoProcessFunction</li>
<li>ProcessJoinFunction</li>
<li>BroadcastProcessFunction</li>
<li>KeyedBroadcastProcessFunction</li>
<li>ProcessWindowFunction</li>
<li>ProcessAllWindowFunction</li>
</ul>
<blockquote>
<p>所有ProcessFunction都基础了RichFunction接口，所以都有open，close，getRuntimeContext()等方法</p>
</blockquote>
<h3 id="KeyedProcessFunction-重点"><a href="#KeyedProcessFunction-重点" class="headerlink" title="KeyedProcessFunction(重点)"></a>KeyedProcessFunction(重点)</h3><p>用来操作KeyedStream。KeyedProcessFunction会处理流中的每个元素，输出0以上的元素。</p>
<p><code>KeyedProcessFunction&lt;K, I, O&gt;</code>的俩个方法</p>
<ul>
<li><code>processElement(I value, Context ctx, Collector&lt;O&gt; out)</code>：处理每个数据都会调用该方法，I为输入数据，out为输出，ctx为上下文</li>
<li><code>onTimer(long timestamp, OnTimerContext ctx, Collector&lt;O&gt; out)</code>：回调函数。定时器触发时的操作。timestamp为定时器所设定的触发时间戳。out为输出。ctx为上下文</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//实现温度连续上升一秒后报警</span></span><br><span class="line"><span class="comment">//泛型为key，输入，输出</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TempIncreAlert</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">String</span>]</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义一个状态，用来保存上一个数据的温度值</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> lastTemp: <span class="type">ValueState</span>[<span class="type">Double</span>] = getRuntimeContext.getState( <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Double</span>](<span class="string">"lastTemp"</span>, classOf[<span class="type">Double</span>]) )</span><br><span class="line">  <span class="comment">// 定义一个状态，用来保存定时器的时间戳</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> currentTimer: <span class="type">ValueState</span>[<span class="type">Long</span>] = getRuntimeContext.getState( <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Long</span>](<span class="string">"currentTimer"</span>, classOf[<span class="type">Long</span>]) )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(value: <span class="type">SensorReading</span>, ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">String</span>]#<span class="type">Context</span>, out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 先取出上一个温度值</span></span><br><span class="line">    <span class="keyword">val</span> preTemp = lastTemp.value()</span><br><span class="line">    <span class="comment">// 更新温度值</span></span><br><span class="line">    lastTemp.update( value.temperature )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> curTimerTs = currentTimer.value()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>( value.temperature &lt; preTemp || preTemp == <span class="number">0.0</span> )&#123;</span><br><span class="line">      <span class="comment">// 如果温度下降，或是第一条数据，删除定时器并清空状态</span></span><br><span class="line">      ctx.timerService().deleteProcessingTimeTimer( curTimerTs )</span><br><span class="line">      currentTimer.clear()</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> ( value.temperature &gt; preTemp &amp;&amp; curTimerTs == <span class="number">0</span> )&#123;</span><br><span class="line">      <span class="comment">// 温度上升且没有设过定时器，则注册定时器，定时器为onTimer内的内容</span></span><br><span class="line">      <span class="keyword">val</span> timerTs = ctx.timerService().currentProcessingTime() + <span class="number">5000</span>L</span><br><span class="line">      ctx.timerService().registerProcessingTimeTimer( timerTs )</span><br><span class="line">      currentTimer.update( timerTs )</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTimer</span></span>(timestamp: <span class="type">Long</span>, ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">String</span>]#<span class="type">OnTimerContext</span>, out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 输出报警信息</span></span><br><span class="line">    out.collect( ctx.getCurrentKey + <span class="string">" 温度连续上升"</span> )</span><br><span class="line">    currentTimer.clear()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="TimerService和定时器"><a href="#TimerService和定时器" class="headerlink" title="TimerService和定时器"></a>TimerService和定时器</h3><p>context和OnTimerContext所有的TimerService对象有这些方法</p>
<ul>
<li><code>currentProcessingTime();</code>：返回当前处理时间</li>
<li><code>currentWatermark();</code>：返回当前watermark的时间戳</li>
<li><code>registerProcessingTimeTimer(long time);</code>：注册当前key的Processing Time定时器，当到达时间时，触发timer</li>
<li><code>registerEventTimeTimer(long time);</code>：注册当前key的event time定时器。当水位线大于等于定时器注册时间时，会触发</li>
<li><code>deleteProcessingTimeTimer(long time);</code>：删除之前处理eventime的定时器，如果没有对应定时器则不执行</li>
<li><code>deleteEventTimeTimer(long time);</code>：删除之前处理Processing Time的定时器，如果没有对应定时器则不执行</li>
</ul>
<h3 id="Emitting-toSide-Outputs-侧输出"><a href="#Emitting-toSide-Outputs-侧输出" class="headerlink" title="Emitting toSide Outputs(侧输出)"></a>Emitting toSide Outputs(侧输出)</h3><p><code>split</code>其实已经被弃用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//冰点报警，如果小于32F，输出报警信息到侧输出流，</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.<span class="type">ProcessFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.timestamps.<span class="type">BoundedOutOfOrdernessTimestampExtractor</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SideOutputTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream = env.socketTextStream(<span class="string">"localhost"</span>, <span class="number">7777</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataStream = stream.map(data =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">      <span class="type">SensorReading</span>(dataArray(<span class="number">0</span>).trim, dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble)</span><br><span class="line">    &#125;)</span><br><span class="line">      .assignTimestampsAndWatermarks( <span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">SensorReading</span>]( <span class="type">Time</span>.seconds(<span class="number">1</span>) ) &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">SensorReading</span>): <span class="type">Long</span> = element.timestamp * <span class="number">1000</span></span><br><span class="line">      &#125; )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> processedStream = dataStream</span><br><span class="line">      .process( <span class="keyword">new</span> <span class="type">FreezingAlert</span>() )</span><br><span class="line"></span><br><span class="line">    processedStream.print(<span class="string">"processed data"</span>)</span><br><span class="line">    <span class="comment">//获取对应的侧输出流，打印</span></span><br><span class="line">    processedStream.getSideOutput( <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">String</span>](<span class="string">"freezing alert"</span>) ).print(<span class="string">"alert data"</span>)</span><br><span class="line">    env.execute(<span class="string">"side output test"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//泛型：输入，主输出</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FreezingAlert</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">ProcessFunction</span>[<span class="type">SensorReading</span>, <span class="type">SensorReading</span>]</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//  lazy val alertOutput: OutputTag[String] = new OutputTag[String]( "freezing alert" )</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(value: <span class="type">SensorReading</span>, ctx: <span class="type">ProcessFunction</span>[<span class="type">SensorReading</span>, <span class="type">SensorReading</span>]#<span class="type">Context</span>, out: <span class="type">Collector</span>[<span class="type">SensorReading</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>( value.temperature &lt; <span class="number">32.0</span> )&#123;<span class="comment">//定义侧输出流</span></span><br><span class="line">      ctx.output( <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">String</span>]( <span class="string">"freezing alert"</span> ), <span class="string">"freezing alert for "</span> + value.id )</span><br><span class="line">    &#125;</span><br><span class="line">    out.collect( value )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="CoProcessFunction"><a href="#CoProcessFunction" class="headerlink" title="CoProcessFunction"></a>CoProcessFunction</h3><p>俩个流，连接在一起。需要提供俩个处理方法，分别对流进行处理</p>
<h2 id="状态管理"><a href="#状态管理" class="headerlink" title="状态管理"></a>状态管理</h2><p>状态：其实就是数据，在处理数据的时候。可能需要依赖，之前的数据中的属性。所有需要保存下来，这就是状态。可以认为是一个本地变量</p>
<p>Flink会统一管理状态。如状态的一致性、故障处理和高效存储</p>
<h3 id="Flink中的状态"><a href="#Flink中的状态" class="headerlink" title="Flink中的状态"></a>Flink中的状态</h3><p>Flink中的状态与特定算子相关联</p>
<p>状态使用前要预先注册</p>
<p>共有俩种类型的状态：</p>
<ul>
<li>算子状态<ul>
<li>算子状态的作用范围限定为算子任务。一个任务一个状态</li>
</ul>
</li>
<li>键控状态<ul>
<li>根据数据定义的key，状态的维护，对应key维护和访问，不同的key有不同的状态。</li>
</ul>
</li>
</ul>
<h3 id="算子状态"><a href="#算子状态" class="headerlink" title="算子状态"></a>算子状态</h3><p>同一并行子任务认为一个状态，同一并行任务的所有数据可以访问到相同的状态。</p>
<p>状态在同一任务是共享，对不同任务是隔离的包括同一算子不同子任务</p>
<h4 id="算子状态数据结构"><a href="#算子状态数据结构" class="headerlink" title="算子状态数据结构"></a>算子状态数据结构</h4><ul>
<li><p>列表状态</p>
<ul>
<li>状态为一组数据的列表</li>
</ul>
</li>
<li><p>联合列表状态</p>
<ul>
<li>与列表状态区别为，在发生故障时，或者从保持点启动程序时如何恢复</li>
</ul>
<blockquote>
<p>列表会根据并行度的调整，重新调整分配。联合列表会将每个状态广播到对应算子</p>
</blockquote>
</li>
<li><p>广播状态</p>
<ul>
<li>一个算子多个子任务的任务状态相同，使用广播状态</li>
</ul>
</li>
</ul>
<blockquote>
<p>实际应用中这些状态用的都不多,用的更多的是none keyed state.没有用键控制的状态.更常见是用keyby后的状态</p>
</blockquote>
<h3 id="键控状态"><a href="#键控状态" class="headerlink" title="键控状态"></a>键控状态</h3><p>一个key会维护自己的状态,不同key访问自己对应的状态.</p>
<p>每条数据,会自动将状态的访问范围限定为当前的key</p>
<h4 id="键控状态数据结构"><a href="#键控状态数据结构" class="headerlink" title="键控状态数据结构"></a>键控状态数据结构</h4><ul>
<li>值状态<ul>
<li>将状态表示为单个的值</li>
</ul>
</li>
<li>列表状态<ul>
<li>一组数据的列表</li>
</ul>
</li>
<li>映射状态<ul>
<li>状态表示为k-v的类型</li>
</ul>
</li>
<li>聚合状态(Reducing state &amp; Aggregating State)<ul>
<li>状态表示为用于聚合创作的列表,每个数据进入,都会被操作</li>
<li>Reducing:内部相当于reduce操作,输出和输入类型同样,不可变</li>
<li>Aggregating:一般的操作.数据类型可不一样</li>
</ul>
</li>
</ul>
<blockquote>
<p>不同数据类型的操作时不同的.具体查看对应文档</p>
</blockquote>
<h5 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//声明一个键控状态.传入相应的名称和类</span></span><br><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> lastTemp: <span class="type">ValueState</span>[<span class="type">Double</span>] = getRuntimeContext.getState( <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Double</span>](<span class="string">"lastTemp"</span>, classOf[<span class="type">Double</span>]) )</span><br><span class="line"><span class="comment">//更新数据</span></span><br><span class="line">lastTemp.update( timerTs )</span><br><span class="line"><span class="comment">//获取值</span></span><br><span class="line"><span class="keyword">val</span> lastTemp = lastTemp.value()</span><br></pre></td></tr></table></figure>

<h3 id="状态后端"><a href="#状态后端" class="headerlink" title="状态后端"></a>状态后端</h3><ul>
<li>管理状态的可插入的组件叫状态后端。是用来管理状态的存储、维护和访问。</li>
<li>主要负责：本地的状态管理、以及将检查点状态写入远程存储</li>
<li>有效的状态访问对于数据的低延迟至关重要，因此每个并行任务都会在本地维护其状态。</li>
</ul>
<h4 id="选择一个状态后端"><a href="#选择一个状态后端" class="headerlink" title="选择一个状态后端"></a>选择一个状态后端</h4><ul>
<li><p>MemoryStateBackend</p>
<ul>
<li>内存级的状态后端，会将键控状态作为对象在内存中管理。将它们存储到TaskManager的JVM堆上，而checkpoint存储在JobManager内存中</li>
</ul>
<blockquote>
<p>快速、低延迟、不稳定。生产环境一般不会使用</p>
</blockquote>
</li>
<li><p>FsStateBackend</p>
<ul>
<li>将checkpoint存到远程的持久化文件系统上，本地状态和MemoryStateBackend一样。</li>
</ul>
<blockquote>
<p>本地拥有内存级的访问。高容错性。</p>
<p>远程存储checkpoint，对性能影响不大</p>
</blockquote>
</li>
<li><p>RocksDBStateBackend</p>
<ul>
<li>将状态序列化存储到本地RocksDB上</li>
</ul>
<blockquote>
<p>状态过大，可能导致oom。就使用该状态后端</p>
</blockquote>
</li>
</ul>
<p>使用RocksDBStateBackend需要引入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-statebackend-rocksdb_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用状态管理</span></span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"路径"</span>))</span><br><span class="line"><span class="comment">//开启checkpoint，传入存储间隔时间</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="状态编程"><a href="#状态编程" class="headerlink" title="状态编程"></a>状态编程</h3><p>在代码中管理状态</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//传入一个阈值，当温度差超过阈值则输出报警。把new出的对象传入flatMap</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TempChangeAlert</span>(<span class="params">threshold: <span class="type">Double</span></span>) <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>[<span class="type">SensorReading</span>, (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> lastTempState: <span class="type">ValueState</span>[<span class="type">Double</span>] = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 初始化的时候声明state变量</span></span><br><span class="line">    lastTempState = getRuntimeContext.getState(<span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Double</span>](<span class="string">"lastTemp"</span>, classOf[<span class="type">Double</span>]))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(value: <span class="type">SensorReading</span>, out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 获取上次的温度值</span></span><br><span class="line">    <span class="keyword">val</span> lastTemp = lastTempState.value()</span><br><span class="line">    <span class="comment">// 用当前的温度值和上次的求差，如果大于阈值，输出报警信息</span></span><br><span class="line">    <span class="keyword">val</span> diff = (value. -temperature lastTemp).abs</span><br><span class="line">    <span class="keyword">if</span> (diff &gt; threshold) &#123;</span><br><span class="line">      out.collect((value.id, lastTemp, value.temperature))</span><br><span class="line">    &#125;</span><br><span class="line">    lastTempState.update(value.temperature)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>flatMapWithState方法。reduce这类方法没有withState，是因为这类方法本身是有状态的</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//用flatMapWithState实现需求。</span></span><br><span class="line"><span class="keyword">val</span> processedStream3 = dataStream.keyBy(_.id)</span><br><span class="line"><span class="comment">//第一个泛型为输出，第二个为状态</span></span><br><span class="line">  .flatMapWithState[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>), <span class="type">Double</span>]&#123;</span><br><span class="line">    <span class="comment">// 如果没有状态的话，也就是没有数据来过，那么就将当前数据温度值存入状态</span></span><br><span class="line">    <span class="keyword">case</span> ( input: <span class="type">SensorReading</span>, <span class="type">None</span> ) =&gt; ( <span class="type">List</span>.empty, <span class="type">Some</span>(input.temperature) )</span><br><span class="line">    <span class="comment">// 如果有状态，就应该与上次的温度值比较差值，如果大于阈值就输出报警</span></span><br><span class="line">    <span class="keyword">case</span> ( input: <span class="type">SensorReading</span>, lastTemp: <span class="type">Some</span>[<span class="type">Double</span>] ) =&gt;</span><br><span class="line">      <span class="keyword">val</span> diff = ( input.temperature - lastTemp.get ).abs</span><br><span class="line">      <span class="keyword">if</span>( diff &gt; <span class="number">10.0</span> )&#123;<span class="comment">//返回一个TraversableOnce[R]的输出数据集。</span></span><br><span class="line">        ( <span class="type">List</span>((input.id, lastTemp.get, input.temperature)), <span class="type">Some</span>(input.temperature) )</span><br><span class="line">      &#125; <span class="keyword">else</span></span><br><span class="line">        ( <span class="type">List</span>.empty, <span class="type">Some</span>(input.temperature) )</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h2><h3 id="一致性检查点"><a href="#一致性检查点" class="headerlink" title="一致性检查点"></a>一致性检查点</h3><ul>
<li>有状态流应用的检查点，就是所有任务在某个时间点的快照备份。这个时间点，应该是所有任务都恰好处理完一个相同的输入数据的时候</li>
<li>Flink故障恢复机制的核心，就是检查点 </li>
</ul>
<blockquote>
<p>让数据流动流动到刚好处理完某一个数据的时候，这时候保存快照</p>
<p>会保存的数据的偏移量</p>
</blockquote>
<h4 id="检查点恢复"><a href="#检查点恢复" class="headerlink" title="检查点恢复"></a>检查点恢复</h4><ul>
<li><p>在执行流应用程序期间，Flink会定期保存状态的一致性检查点</p>
</li>
<li><p>如果发生故障，Flink将会使用最近的检查点来一致恢复应用程序的状态，并重新启动处理流程（重启处理流程，可以由我们在代码中或配置文件中配置）</p>
</li>
<li><p>这种检查点机制，保证了应用程序状态“精确一次”的一致性。所有算子都会保存检查点并恢复所有状态，这样所有的输入流会被重置到检查点完成时</p>
</li>
</ul>
<blockquote>
<p>精确一次就是数据保证不丢失，只处理一次。这里的精确一次是内部的精确一次。无法保证外部精确一次。</p>
</blockquote>
<h4 id="检查点实现算法和分界线"><a href="#检查点实现算法和分界线" class="headerlink" title="检查点实现算法和分界线"></a>检查点实现算法和分界线</h4><ul>
<li>简单的实现<ul>
<li>暂停所有应用，保存状态到检查点，再重新恢复</li>
<li>性能损耗过大</li>
</ul>
</li>
<li>Flink的实现<ul>
<li>改进基于Chandy-Lamport算法的分布式快照</li>
<li>将检查点的保存和数据处理分离开，不暂停整个应用。只短暂暂停对应的应用</li>
</ul>
</li>
</ul>
<p>检查点分界线</p>
<ul>
<li>Flink的检查点算法通过分界线，将一条流上数据按照不同的检查点分开<ul>
<li>分界线是一种特殊数据形式</li>
</ul>
</li>
<li>分界线之前到来的数据导致的状态更改，都会保存到当前分界线所属的检查点中。分界线后的数据更改，会保存到之后的检查点</li>
</ul>
<h3 id="检查点流程"><a href="#检查点流程" class="headerlink" title="检查点流程"></a>检查点流程</h3><p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/checkpoint_1.png" alt></p>
<p>该图共分为：输入、根据奇偶重排，分别计算奇数和偶数和、输出</p>
<p>有俩个流并行运作</p>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/checkpoint_2.png" alt></p>
<p>JobManager向每个source任务发送带有新检查点ID的消息，通过这种方式，启动检查点</p>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/checkpoint_3.png" alt></p>
<ol>
<li>收到检查点，任务会停止然后数据源会将它们的状态写入检查点，并会发出检查点barrier，</li>
<li>在存入检查点之后，会返回通知给source任务，source任务会向JobManager确认检查点完成</li>
</ol>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/checkpoint_4.png" alt></p>
<p>barrier会向下游传递，sum任务会等待所有输入分区的barrier到达，比如黄色检查点到达后，黄色的数据再来就会被存入缓存。还未到达的检查点的分区数据会继续处理</p>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/checkpoint_5.png" alt></p>
<p>所有输入分区的barrier都到达后，任务就将状态保存到转台后端的检查点。然后将barrier向下发送</p>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/checkpoint_6.png" alt></p>
<p>发送后，任务继续正常处理</p>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/checkpoint_7.png" alt></p>
<p>Sink任务向JobManager确认状态保存到checkpoint完毕。</p>
<p>当所有任务都确认成功保存，检查点建立完毕</p>
<h4 id="保存点"><a href="#保存点" class="headerlink" title="保存点"></a>保存点</h4><p>原理和算法几乎和检查点一样。用法不同</p>
<p>Flink不会自动创建保持点，需要用户指定触发创建。</p>
<p>保持点，除了故障恢复外，可用于：手动备份，更新升级应用程序，版本迁移，测试，展厅和重启应用，也可以升级Flink版本，暂停不重要Flink程序释放资源等等</p>
<h3 id="检查点配置"><a href="#检查点配置" class="headerlink" title="检查点配置"></a>检查点配置</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//检查点间隔</span></span><br><span class="line">env.enableCheckpointing(<span class="number">60000</span>)</span><br><span class="line"><span class="comment">//默认EXACTLY_ONCE，可以改成至少一次</span></span><br><span class="line">env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">AT_LEAST_ONCE</span>)</span><br><span class="line"><span class="comment">//超时时间，超过就丢弃checkpoint</span></span><br><span class="line">env.getCheckpointConfig.setCheckpointTimeout(<span class="number">100000</span>)</span><br><span class="line"><span class="comment">//checkpoint失败，是否整个任务失败，默认是true。</span></span><br><span class="line">env.getCheckpointConfig.setFailOnCheckpointingErrors(<span class="literal">false</span>)</span><br><span class="line"><span class="comment">//最大同时的checkpoint的数量，默认为1</span></span><br><span class="line">env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line"><span class="comment">//俩次checkpoint最小时间间隔，如果设置了间隔，最大同时数就只能为1</span></span><br><span class="line">env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">100</span>)</span><br><span class="line"><span class="comment">//开启checkpoint外部持久化。如果不设置，job失败后checkpoint会清理掉checkpoint的持久化。</span></span><br><span class="line"><span class="comment">// DELETE_ON_CANCELLATION:手动取消就不要存,RETAIN_ON_CANCELLATION：即使手动取消也要存</span></span><br><span class="line">env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">DELETE_ON_CANCELLATION</span>)</span><br><span class="line"><span class="comment">//重启策略，</span></span><br><span class="line"><span class="comment">//fixedDelayRestart：尝试次数、尝试间隔时间</span></span><br><span class="line"><span class="comment">// failureRateRestart：失败率最大尝试次数在失败尝试时间内、失败尝试时间、尝试间隔</span></span><br><span class="line">env.setRestartStrategy(<span class="type">RestartStrategies</span>.failureRateRestart(<span class="number">3</span>, org.apache.flink.api.common.time.<span class="type">Time</span>.seconds(<span class="number">300</span>), org.apache.flink.api.common.time.<span class="type">Time</span>.seconds(<span class="number">10</span>)))</span><br></pre></td></tr></table></figure>

<h2 id="状态一致性"><a href="#状态一致性" class="headerlink" title="状态一致性"></a>状态一致性</h2><p>无论是断电、短路、宕机等等，最后结果都要准确</p>
<ul>
<li>一条数据不应该丢失，也不能重复计算</li>
<li>遇到故障可以恢复，恢复后重新计算结果也是正确</li>
</ul>
<h3 id="状态一致性的类别"><a href="#状态一致性的类别" class="headerlink" title="状态一致性的类别"></a>状态一致性的类别</h3><ul>
<li>AT-MOST-ONCE（最多一次）<ul>
<li>当任务故障时，啥事都不干，既不恢复丢失的状态，也不重播丢失的数据。</li>
</ul>
</li>
<li>AT-LEAST-ONCE（至少一次）<ul>
<li>所有事件都得到处理，可能会被处理多次</li>
</ul>
</li>
<li>EXACTLY-ONCE（精确一次）<ul>
<li>恰好处理一次，保证不丢失</li>
</ul>
</li>
</ul>
<blockquote>
<p>一致性检查点是Flink数据一致性的的保证</p>
</blockquote>
<h3 id="端到端状态一致性"><a href="#端到端状态一致性" class="headerlink" title="端到端状态一致性"></a>端到端状态一致性</h3><p>在生产环境中，需要保证数据源、流处理器和输出到持久化系统的数据一致性。</p>
<p>整个端到端的数据一致性取决于所有组件最弱的一个组件</p>
<h4 id="端到端exactly-once"><a href="#端到端exactly-once" class="headerlink" title="端到端exactly-once"></a>端到端exactly-once</h4><ul>
<li>内部保证—checkpoint</li>
<li>source端—可重设数据的读取位置</li>
<li>sink端—从故障恢复时，数据不会重复写入外部系统<ul>
<li>幂等写入</li>
<li>事物写入</li>
</ul>
</li>
</ul>
<h4 id="幂等写入-Idempotent-Writes"><a href="#幂等写入-Idempotent-Writes" class="headerlink" title="幂等写入(Idempotent Writes)"></a>幂等写入(Idempotent Writes)</h4><p>幂等操作，就是一个操作执行多次和执行一次的结果是一致的</p>
<blockquote>
<p>幂等写入无法做到，真正意义上的精确一次</p>
</blockquote>
<h4 id="事务写入"><a href="#事务写入" class="headerlink" title="事务写入"></a>事务写入</h4><p>所有做错必须同时成功或者同时失败</p>
<p>实现思想：构建的事物对应着checkpoint，等到checkpoint完成的时候，才将结果写入sink</p>
<p>实现方式</p>
<ul>
<li>预写日志</li>
<li>两阶段提交</li>
</ul>
<h5 id="预写日志-WAL"><a href="#预写日志-WAL" class="headerlink" title="预写日志(WAL)"></a>预写日志(WAL)</h5><ul>
<li>把结果数据先当初状态保持，在收到checkpoint完成通知后，一次性写入sink</li>
</ul>
<p>DataStream API提供了模板类：GenericWriteAheadSink来实现这种事务性sink</p>
<p>可能会出现最后一次性写入，出现故障。再重写可能会导致数据重复</p>
<h4 id="两阶段提交-2pc"><a href="#两阶段提交-2pc" class="headerlink" title="两阶段提交(2pc)"></a>两阶段提交(2pc)</h4><ul>
<li>对于每个checkpoint，sink任务都会启动事物，然后将数据添加到事务中</li>
<li>然后将数据写到外部sink系统，但不提交，只是”预提交”</li>
<li>等他收到checkpoint完成通知时，才正式提交事务，真正写入</li>
</ul>
<blockquote>
<p>实现了exactly-one，但是需要一个提供事务支持的外部sink系统。Flink提供了TwoPhaseCommitSinkFunction接口</p>
</blockquote>
<h5 id="2pc对外部sink系统的要求"><a href="#2pc对外部sink系统的要求" class="headerlink" title="2pc对外部sink系统的要求"></a>2pc对外部sink系统的要求</h5><ul>
<li><p>外部sink系统必须提供事务支持，或sink任务必须能够模拟外部系统上的事务</p>
</li>
<li><p>在checkpoint间隔间内，必须能够开启一个事务接收数据写入</p>
</li>
<li><p>在收到checkpoint完成通知之前，事务必须是”等待提交”的出台。如果超时，未提交数据会丢失</p>
</li>
<li><p>sink任务能在进程失败后恢复事务</p>
</li>
</ul>
<p>提交事务必须是幂等操作</p>
<p><img src="/2019/11/26/Flink%E7%AC%94%E8%AE%B0/e2e.png" alt></p>
<h3 id="Flink-Kafka端到端状态一致性"><a href="#Flink-Kafka端到端状态一致性" class="headerlink" title="Flink+Kafka端到端状态一致性"></a>Flink+Kafka端到端状态一致性</h3><ul>
<li><p>source：kafka consumer作为source，可以将偏移量保留，如果后续任务出故障，恢复可以由连接器重置偏移量，重新消费</p>
</li>
<li><p>transform：checkpoint机制</p>
</li>
<li><p>sink：kafka producer作为sink，采用两阶段提交sink，实现一个TwoPhaseCommitSinkFunction</p>
</li>
</ul>
<p>提交步骤</p>
<ol>
<li>第一条数据来了就开启了kafka的一个事务，正常写入kafka分区日志，但标记未提交</li>
<li>jobmanager触发checkpoint操作，barrier从source开始向下传递，遇到barrier的算子将状态存入状态后端，并通知jobmanager</li>
<li>sink连接器收到barrier，保存当前状态，存入checkpoint，通知jobmanager，并开启下一段事务</li>
<li>jobmanager收到所有任务的通知，发出确认信息，表示checkpoint完成</li>
<li>sink任务收到jobmanager的确认信息，表示checkpoint完成</li>
<li>kafka关闭事务，已提交的数据可以正常消费</li>
</ol>
<blockquote>
<p>flink的sink超时时间默认为1小时</p>
<p>kafka的事件默认超时时间15分钟</p>
</blockquote>
<h2 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h2><p>Table API是流处理和批处理通用的关系型API，Table API可以基于流输入或者批输入来运行而不需要进行任何修改。Table API是SQL语言的超集并专门为Apache Flink设计的，Table API是Scala 和Java语言集成式的API。与常规SQL语言中将查询指定为字符串不同，Table API查询是以Java或Scala中的语言嵌入样式来定义的，具有IDE支持如:自动完成和语法检测。</p>
<h3 id="快速使用"><a href="#快速使用" class="headerlink" title="快速使用"></a>快速使用</h3><p>pom</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>样例</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> myKafkaConsumer: <span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>] = <span class="type">MyKafkaUtil</span>.getConsumer(<span class="string">"ECOMMERCE"</span>)</span><br><span class="line">    <span class="keyword">val</span> dstream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.addSource(myKafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> startupLogDstream: <span class="type">DataStream</span>[<span class="type">StartupLog</span>] = dstream.map&#123; jsonString =&gt;<span class="type">JSON</span>.parseObject(jsonString,classOf[<span class="type">StartupLog</span>]) &#125;</span><br><span class="line">    <span class="comment">//从dataStream读取一张表</span></span><br><span class="line">    <span class="keyword">val</span> startupLogTable: <span class="type">Table</span> = tableEnv.fromDataStream(startupLogDstream)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> table: <span class="type">Table</span> = startupLogTable.select(<span class="string">"mid,ch"</span>).filter(<span class="string">"ch ='appstore'"</span>)</span><br><span class="line">    <span class="comment">//dataStream转化为流</span></span><br><span class="line">    <span class="keyword">val</span> midchDataStream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = table.toAppendStream[(<span class="type">String</span>,<span class="type">String</span>)]</span><br><span class="line"></span><br><span class="line">    midchDataStream.print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="动态表"><a href="#动态表" class="headerlink" title="动态表"></a>动态表</h3><p>如果流中的数据类型是case classk可以总结根据case class的结构生成table</p>
<p><code>tableEnv.fromDataStream(ecommerceLogDstream)</code></p>
<p>或者根据字段顺序单独命名<code>tableEnv.fromDataStream(ecommerceLogDstream,&#39;mid&#39;,&#39;name&#39;,……)</code></p>
<p>动态表可以转化为流</p>
<p><code>table.toAppendStream[(String,String)]</code></p>
<blockquote>
<p>传入的字段名为单引号，Append模式只能用insert</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//每10秒中渠道为appstore的个数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">//sparkcontext</span></span><br><span class="line">  <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">  <span class="comment">//时间特性改为eventTime</span></span><br><span class="line">  env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> myKafkaConsumer: <span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>] = <span class="type">MyKafkaUtil</span>.getConsumer(<span class="string">"GMALL_STARTUP"</span>)</span><br><span class="line">  <span class="keyword">val</span> dstream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.addSource(myKafkaConsumer)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> startupLogDstream: <span class="type">DataStream</span>[<span class="type">StartupLog</span>] = dstream.map&#123; jsonString =&gt;<span class="type">JSON</span>.parseObject(jsonString,classOf[<span class="type">StartupLog</span>]) &#125;</span><br><span class="line">  <span class="comment">//告知watermark 和 eventTime如何提取</span></span><br><span class="line">  <span class="keyword">val</span> startupLogWithEventTimeDStream: <span class="type">DataStream</span>[<span class="type">StartupLog</span>] = startupLogDstream.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">StartupLog</span>](<span class="type">Time</span>.seconds(<span class="number">0</span>L)) &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">StartupLog</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">      element.ts</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;).setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//SparkSession</span></span><br><span class="line">  <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//把数据流转化成Table</span></span><br><span class="line">  <span class="comment">//时间使用eventtime要用.rowtime</span></span><br><span class="line">  <span class="comment">//如果为Processing Time，就要自己指定字段.proctime</span></span><br><span class="line">  <span class="keyword">val</span> startupTable: <span class="type">Table</span> = tableEnv.fromDataStream(startupLogWithEventTimeDStream , <span class="symbol">'mid</span>,<span class="symbol">'uid</span>,<span class="symbol">'appid</span>,<span class="symbol">'area</span>,<span class="symbol">'os</span>,<span class="symbol">'ch</span>,<span class="symbol">'logType</span>,<span class="symbol">'vs</span>,<span class="symbol">'logDate</span>,<span class="symbol">'logHour</span>,<span class="symbol">'logHourMinute</span>,<span class="symbol">'ts</span>.rowtime)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//通过table api 进行操作</span></span><br><span class="line">  <span class="comment">// 每10秒 统计一次各个渠道的个数 table api 解决</span></span><br><span class="line">  <span class="comment">//1 groupby  2 要用 window   3 用eventtime来确定开窗时间</span></span><br><span class="line">  <span class="keyword">val</span> resultTable: <span class="type">Table</span> = startupTable.window(<span class="type">Tumble</span> over <span class="number">10000.</span>millis on <span class="symbol">'ts</span> as <span class="symbol">'tt</span>).groupBy(<span class="symbol">'ch</span>,<span class="symbol">'tt</span> ).select( <span class="symbol">'ch</span>, <span class="symbol">'ch</span>.count)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//把Table转化成数据流</span></span><br><span class="line">  <span class="comment">//val appstoreDStream: DataStream[(String, String, Long)] = appstoreTable.toAppendStream[(String,String,Long)]</span></span><br><span class="line">  <span class="keyword">val</span> resultDstream: <span class="type">DataStream</span>[(<span class="type">Boolean</span>, (<span class="type">String</span>, <span class="type">Long</span>))] = resultSQLTable.toRetractStream[(<span class="type">String</span>,<span class="type">Long</span>)]</span><br><span class="line"></span><br><span class="line">  resultDstream.filter(_._1).print()</span><br><span class="line"></span><br><span class="line">  env.execute()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//窗口函数，preAggregator做聚合运算。windowFunction做结果输出</span></span><br><span class="line"><span class="comment">//传入的俩个函数，可以继承自定义</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate</span></span>[<span class="type">ACC</span>: <span class="type">TypeInformation</span>, <span class="type">V</span>: <span class="type">TypeInformation</span>, <span class="type">R</span>: <span class="type">TypeInformation</span>](</span><br><span class="line">      <span class="comment">//IN：输入，OUT：输出，ACC累加器(中间的聚合状态)</span></span><br><span class="line">      preAggregator: <span class="type">AggregateFunction</span>[<span class="type">IN</span>, <span class="type">ACC</span>, <span class="type">OUT</span>],</span><br><span class="line">      <span class="comment">//IN：输入(这里的输入一般是聚合函数的输出)，OUT：输出，KEY：key的类型*(具体按照keyby返回的类型)，W：当前window类型</span></span><br><span class="line">      windowFunction: <span class="type">WindowFunction</span>[<span class="type">IN</span>, <span class="type">OUT</span>, <span class="type">KEY</span>, <span class="type">W</span> &lt;: <span class="type">Window</span>]): <span class="type">DataStream</span>[<span class="type">R</span>]</span><br></pre></td></tr></table></figure>

</div><div class="tags"><a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"><i class="fa fa-tag"></i>大数据</a><a href="/tags/Flink/"><i class="fa fa-tag"></i>Flink</a></div><div class="post-nav"><a class="pre" href="/2019/11/27/Shell%E7%BC%96%E7%A8%8B/">Shell编程</a><a class="next" href="/2019/11/10/ElasticSearch%E7%AC%94%E8%AE%B0/">ElasticSearch笔记</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://example.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/%E5%B9%B6%E5%8F%91/" style="font-size: 15px;">并发</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/nosql/" style="font-size: 15px;">nosql</a> <a href="/tags/elasticsearch/" style="font-size: 15px;">elasticsearch</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/PHP/" style="font-size: 15px;">PHP</a> <a href="/tags/%E5%90%8E%E7%AB%AF/" style="font-size: 15px;">后端</a> <a href="/tags/redis/" style="font-size: 15px;">redis</a> <a href="/tags/scala/" style="font-size: 15px;">scala</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 15px;">大数据</a> <a href="/tags/SQL/" style="font-size: 15px;">SQL</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/springsecurity/" style="font-size: 15px;">springsecurity</a> <a href="/tags/php/" style="font-size: 15px;">php</a> <a href="/tags/wordpress/" style="font-size: 15px;">wordpress</a> <a href="/tags/C/" style="font-size: 15px;">C++</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/marjora/" style="font-size: 15px;">marjora</a> <a href="/tags/virtualbox/" style="font-size: 15px;">virtualbox</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/bug/" style="font-size: 15px;">bug</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 15px;">服务器</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 15px;">博客</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" style="font-size: 15px;">计算机基础</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 15px;">爬虫</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 15px;">设计模式</a> <a href="/tags/%E6%B5%81%E7%A8%8B/" style="font-size: 15px;">流程</a> <a href="/tags/%E5%BC%80%E5%8F%91/" style="font-size: 15px;">开发</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a> <a href="/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">并发编程</a> <a href="/tags/spring/" style="font-size: 15px;">spring</a> <a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 15px;">前端</a> <a href="/tags/Vue/" style="font-size: 15px;">Vue</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 15px;">数据结构</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/07/06/Redis/">Redis</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/24/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/">Git笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/07/JVM/">JVM</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/27/%E7%88%AC%E8%99%AB/">爬虫</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/">计算机组成原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/Git%E7%AC%94%E8%AE%B0/">Git笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/manjaro%E9%85%8D%E7%BD%AE%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/">manjaro配置开发环境</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/docker%E7%AC%94%E8%AE%B0/">docker笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/13/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">dian的博客.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>