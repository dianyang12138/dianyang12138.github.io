<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="后端Java工程师"><title>SparkCore | dian的博客</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 4.2.1"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">SparkCore</h1><a id="logo" href="/.">dian的博客</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">SparkCore</h1><div class="post-meta">2019-10-30</div><div class="post-content"><a id="more"></a>

<h2 id="RDD概述"><a href="#RDD概述" class="headerlink" title="RDD概述"></a>RDD概述</h2><p><img src="/2019/10/30/SparkCore/RDD_mold.png" alt></p>
<p>只有在collect的时候才会真正开始运行，前面都只是封装逻辑</p>
<h3 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h3><p>RDD（Resilient Distributed<br>Dataset）叫做分布式数据集，是Spark中最基本的数据抽象。代码中是一个抽象类，它代表一个不可变、可分区、里面的元素可并行计算的集合。</p>
<blockquote>
<p>分布式是指数据来源为分布式，可能来自多个文件</p>
</blockquote>
<h4 id="RDD的属性"><a href="#RDD的属性" class="headerlink" title="RDD的属性"></a>RDD的属性</h4><ul>
<li><p>一组分区（Partition），即数据集的基本组成单位;</p>
</li>
<li><p>一个计算每个分区的函数;</p>
</li>
<li><p>RDD之间的依赖关系;（装饰者模式，血缘）</p>
</li>
<li><p>一个Partitioner，即RDD的分片函数;</p>
</li>
<li><p>一个列表，存储存取每个Partition的优先位置（数据存储的位置）（preferred location）。（移动数据不如移动计算）</p>
</li>
</ul>
<blockquote>
<p>首选位置是一个seq，表示优先同一个进程，其次同一个机器，再其次同一个机架</p>
</blockquote>
<h4 id="RDD特点"><a href="#RDD特点" class="headerlink" title="RDD特点"></a>RDD特点</h4><p>RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系<strong>延时</strong>计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。</p>
<h4 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h4><p>RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。</p>
<blockquote>
<p>就是可以并行计算</p>
</blockquote>
<h4 id="只读"><a href="#只读" class="headerlink" title="只读"></a>只读</h4><p>RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。</p>
<p>由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了</p>
<h5 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h5><p>从认知心理学角度，解决问题其实是将问题的初始化状态，通过一系列的操作（算子）对问题的状态进行转化，然后达成完成（解决）状态。</p>
<p>Spark中的所有的RDD方法都称之为算子，但是分为2大类：转化算子&amp;行动算子</p>
<h3 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h3><p>创建RDD方式分为3种：<strong>从集合中创建</strong>；<strong>从外部存储创建</strong>；从其他RDD创建</p>
<h4 id="从集合中创建"><a href="#从集合中创建" class="headerlink" title="从集合中创建"></a>从集合中创建</h4><p>从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br></pre></td></tr></table></figure>

<h4 id="从外部存储创建"><a href="#从外部存储创建" class="headerlink" title="从外部存储创建"></a>从外部存储创建</h4><p>包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd2= sc.textFile(<span class="string">"hdfs://hadoop102:9000/RELEASE"</span>)</span><br></pre></td></tr></table></figure>

<h4 id="textFile-amp-makeRDD区别"><a href="#textFile-amp-makeRDD区别" class="headerlink" title="textFile&amp;makeRDD区别"></a>textFile&amp;makeRDD区别</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//分区数，如果没指定就是getmin(可用Cpu核数,2)</span></span><br><span class="line"><span class="comment">//读取文件时，传递的分区参数为最小分区数，但是不一定是这个分区数，取决于Hadoop读取文件时分片规则</span></span><br><span class="line">sc.textFile(<span class="string">"in"</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">//4，先看有没有默认值，如果没有getmax(可用的Cpu核数,2)</span></span><br><span class="line"><span class="comment">//Local模式的时候，没有配置就是local[*]就是本机最大内核数</span></span><br><span class="line"><span class="comment">//多余的数放最后一个分区，老师是这么说的，但是我验证是平均随机的</span></span><br><span class="line">sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<h3 id="RDD的转换（面试开发重点）"><a href="#RDD的转换（面试开发重点）" class="headerlink" title="RDD的转换（面试开发重点）"></a>RDD的转换（面试开发重点）</h3><h4 id="Value类型"><a href="#Value类型" class="headerlink" title="Value类型"></a>Value类型</h4><p><code>map(func)</code>：一个一个进行执行。</p>
<p><code>mapPartitions(func)</code>：一个分区一个分区的执行。效率优于map</p>
<blockquote>
<p>一个func是一个算子。因为map细化，所有发送执行的次数高，效率就低。mapPartitions只需要发给相应分区就行了。交互次数小，如果分区大但是可能爆内存（oom）。</p>
</blockquote>
<p><code>mapPartitionsWithIndex(func)</code>：类似于mapPartitions，传值要传(Int, Interator[T]) =&gt; Iterator[U]</p>
<p><code>flatMap(func)</code>：类似于map，不会扁平化(如果是RDD[List[Int]]不会扁平化为Int传入)，但是是输入一个元素，返回一个序列（0至多个元素）。但是存入还是单个元素的存。最后是一个一个的个体</p>
<p><code>glom()</code>：将每一个分区转成一个数组，返回。</p>
<p><code>groupBy(func)</code>：分组，按照传入函数的返回值进行分组。将相同的key对应的值放入一个迭代器。分组后的数据形成了对偶元组（K-V），K表示分组的key，v表示分组的数据集合</p>
<p><code>sample(withReplacement,fraction, seed)</code>：随机抽样。数量为fraction（false：0-1，true：可以大于1就会重复了）的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。</p>
<p><code>distinct([numTasks]))</code>：对源RDD进行去重后返回一个新的RDD（数据打乱重组）。默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。</p>
<blockquote>
<p>rdd一个分区的数据打乱重组到其他不同分区的操作，称之为shuffle。</p>
<p>俩个分区合并一起，不叫shuffle</p>
<p>shuffle需要写，影响性能</p>
</blockquote>
<p><code>coalesce(numPartitions)</code>：减少分区数（合并分区）。默认shuffle是false，可以改为true。重新洗牌。无shuffle容易引起数据倾斜。</p>
<p><code>repartition(numPartitions)</code>：根据分区数，重新通过网络随机洗牌所有数据。实际上是调用的coalesce</p>
<p><code>sortBy(func,[ascending],[numTasks])</code>：使用func先对数据进行处理，按照处理后的数据比较结果排序，默认为正序。<code>ascending: Boolean = false</code>逆序</p>
<h4 id="双Value类型交互"><a href="#双Value类型交互" class="headerlink" title="双Value类型交互"></a>双Value类型交互</h4><p><code>union(otherDataset)</code>：对源RDD和参数RDD求并集后返回一个新的RDD。<code>RDD1.union(RDD2)</code></p>
<p><code>subtract (otherDataset)</code>：去除两个RDD中相同的元素，不同的RDD将保留下来</p>
<p><code>intersection(otherDataset)</code>：求交集后返回一个新的RDD</p>
<p><code>cartesian(otherDataset)</code>：笛卡尔积（少用）</p>
<p><code>zip(otherDataset)</code>：将两个RDD组合成Key/Value形式的RDD,这里默认两个RDD的partition数量以及元素数量都相同，否则会抛出异常。</p>
<h4 id="Key-Value类型"><a href="#Key-Value类型" class="headerlink" title="Key-Value类型"></a>Key-Value类型</h4><p><code>partitionBy</code>：对pairRDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则就会产生shuffle过程。传一个<code>org.apache.spark.Partitioner</code>.也可以自己继承实现。只要k-v的RDD才可以，这里用到了隐式转换</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span>(<span class="params">partitions: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">  <span class="comment">//分区数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = &#123;</span><br><span class="line">    partitions</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//返回去哪个分区</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>groupByKey</code>：分组更具key，但只生成一个sequence。</p>
<p><code>reduceByKey(func,[numTasks])</code>：根据分组Key，将俩个Value传入，返回一个Value。第二个指定任务数（并行数）</p>
<blockquote>
<p>在shuffle之前有combine（预聚合）操作。写的量会低一些</p>
</blockquote>
<p><code>aggregateByKey(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U)</code> ：zeroValue给每个key一个初始值（第一次用），seqOp：分区内函数，combOp：分区间函数。</p>
<blockquote>
<p>取出每个分区相同key对应值的最大值，然后相同key相加 <code>rdd.aggregateByKey(0)(math.max(_,_),_+_)</code></p>
</blockquote>
<p><code>foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]</code>：可以理解为，与aggregateByKey差不多。分区内和分区间用相同的函数。和aggregateByKey底层用的相同的函数</p>
<p><code>combineByKey[C](createCombiner: V =&gt; C,  mergeValue: (C, V) =&gt; C,  mergeCombiners: (C, C) =&gt; C)</code>createCombiner :分区内每种key调用一次，mergeValue：分区内将createCombiner()结果与相同的key对应的值做合并，mergeCombiners：分区间相同的key做聚合</p>
<blockquote>
<p>combineByKey如果不加参数的类型可能会报错，aggregateByKey中不会，因为可以通过运行时反射获得</p>
</blockquote>
<p> <code>sortByKey([ascending],[numTasks])</code>：在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</p>
<p><code>mapValues(f: V =&gt; U)</code>：只对V进行操作</p>
<p><code>join(otherDataset,[numTasks])</code>：：在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(V,W))的RDD。性能低</p>
<p><code>cogroup(otherDataset,[numTasks])</code></p>
<h3 id="行动算子（Action）"><a href="#行动算子（Action）" class="headerlink" title="行动算子（Action）"></a>行动算子（Action）</h3><p><code>reduce(func)</code>：聚集RDD中的所有元素，先聚合分区内数据，再聚合分区间数据。函数接受俩个值，返回一个</p>
<p><code>collect()</code>：以数组的形式返回数据集的所有元素。</p>
<p><code>count()</code>：返回RDD中元素的个数</p>
<p><code>first()</code>：返回RDD中的第一个元素</p>
<p><code>take(n)</code>：返回一个由RDD的前n个元素组成的数组</p>
<p><code>takeOrdered(n)</code>：返回该RDD排序后的前n个元素组成的数组</p>
<p><code>aggregate(zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)</code>：ggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。</p>
<blockquote>
<p>这里分区内加初始值，分区间也会加初始值。但是之前的aggregateByKey只会在分区内加初始值</p>
</blockquote>
<p><code>fold(num)(func)</code>：折叠操作，aggregate的简化操作，seqop和combop一样。</p>
<p><code>saveAsTextFile(path)</code>：将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</p>
<p><code>saveAsSequenceFile(path)</code>：将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。</p>
<p><code>saveAsObjectFile(path)</code>：用于将RDD中的元素序列化成对象，存储到文件中。</p>
<p><code>countByKey()</code>：针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</p>
<p><code>foreach(func)</code>：在数据集的每一个元素上，运行函数func进行更新。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Executor中执行</span></span><br><span class="line">rdd.foreach(</span><br><span class="line">	i=&gt;&#123;</span><br><span class="line">        println(i*<span class="number">2</span>)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"><span class="comment">//Drvier中执行</span></span><br><span class="line">rdd.collect.foreach(</span><br><span class="line">    i=&gt;&#123;</span><br><span class="line">        println(i*<span class="number">2</span>)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="Driver和Executor的关系"><a href="#Driver和Executor的关系" class="headerlink" title="Driver和Executor的关系"></a>Driver和Executor的关系</h3><p>只有算子是在Executor种执行的，如果算子中用到了Driver中的资源，则该资源必须被序列化，否则会报错。</p>
<p><img src="/2019/10/30/SparkCore/Drive&Executor.png" alt></p>
<h4 id="RDD中的函数传递"><a href="#RDD中的函数传递" class="headerlink" title="RDD中的函数传递"></a>RDD中的函数传递</h4><p>在实际开发中我们往往需要自己定义一些对于RDD的操作，那么此时需要主要的是，初始化工作是在Driver端进行的，而实际运行程序是在Executor端进行的，这就涉及到了跨进程通信，是需要序列化的。</p>
<p>使类继承scala.Serializable即可</p>
<blockquote>
<p>如果Executor执行的代码段，用到的是局部变量，而不是对象属性就不用实例化i</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SeriTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.初始化配置信息及SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"WordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.创建一个RDD</span></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.parallelize(<span class="type">Array</span>(<span class="string">"hadoop"</span>, <span class="string">"spark"</span>, <span class="string">"hive"</span>, <span class="string">"atguigu"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.创建一个Search对象</span></span><br><span class="line">    <span class="keyword">val</span> search = <span class="keyword">new</span> <span class="type">Search</span>(<span class="string">"h"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//4.运用第一个过滤函数并打印结果</span></span><br><span class="line">    <span class="keyword">val</span> match1: <span class="type">RDD</span>[<span class="type">String</span>] = search.getMatche1(rdd)</span><br><span class="line">    match1.collect().foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Search</span>(<span class="params">s:<span class="type">String</span></span>)</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//过滤出包含字符串的数据</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    s.contains(query)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatch1</span> </span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    rdd.filter(isMatch)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatche2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    rdd.filter(x =&gt; x.contains(query))</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//过滤出包含字符串的RDD，这样就不会报错</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatche2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> query_ : <span class="type">String</span> = <span class="keyword">this</span>.query<span class="comment">//将类变量赋值给局部变量</span></span><br><span class="line">    rdd.filter(x =&gt; x.contains(query_))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="RDD依赖关系"><a href="#RDD依赖关系" class="headerlink" title="RDD依赖关系"></a>RDD依赖关系</h3><p>将创建RDD的一系列Lineage（血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p>
<blockquote>
<p>RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。</p>
</blockquote>
<h4 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h4><p>窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用,窄依赖我们形象的比喻为独生子女</p>
<p><img src="/2019/10/30/SparkCore/narrow_dependency.jpg" alt></p>
<h4 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h4><p>宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition，会引起shuffle,总结：宽依赖我们形象的比喻为超生</p>
<p><img src="/2019/10/30/SparkCore/wide_dependency.jpg" alt></p>
<h4 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h4><p>DAG(Directed Acyclic Graph)叫做有向无环图。</p>
<p>根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此<strong>宽依赖是划分Stage的依据</strong>。</p>
<p><img src="/2019/10/30/SparkCore/DAG.jpg" alt></p>
<h4 id="任务划分（面试重点）"><a href="#任务划分（面试重点）" class="headerlink" title="任务划分（面试重点）"></a>任务划分（面试重点）</h4><ol>
<li>Application：初始化一个SparkContext即生成一个Application</li>
<li>Job：一个（行动）Action算子就会生成一个Job</li>
<li>Stage：根据RDD之间的依赖关系的不同将Job划分成不同的Stage，遇到一个宽依赖则划分一个Stage。</li>
<li>Task：Stage是一个TaskSet，将Stage划分的结果发送到不同的Executor执行即为一个Task。</li>
</ol>
<blockquote>
<p>Application-&gt;Job-&gt;Stage-&gt; Task每一层都是1对n的关系。</p>
</blockquote>
<p><img src="/2019/10/30/SparkCore/wordcount.png" alt></p>
<h3 id="RDD缓存"><a href="#RDD缓存" class="headerlink" title="RDD缓存"></a>RDD缓存</h3><p>RDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下 persist() 会把数据以序列化的形式缓存在 JVM 的堆空间中。 </p>
<p>这两个方法在触发后面的action时才会缓存。<img src="/2019/10/30/SparkCore/persist&cache.png" alt></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;</span><br><span class="line">    <span class="comment">//不存</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">    <span class="comment">//磁盘俩分</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">    <span class="comment">//内存 序列化 俩分</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment">//堆外内存</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/2019/10/30/SparkCore/StorageLevel.png" alt></p>
<h4 id="RDD-CheckPoint"><a href="#RDD-CheckPoint" class="headerlink" title="RDD CheckPoint"></a>RDD CheckPoint</h4><p>park中对于数据的保存除了持久化操作之外，还提供了一种检查点的机制，检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。检查点通过将数据写入到HDFS文件系统实现了RDD的检查点功能。</p>
<p>为当前RDD设置检查点。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用SparkContext.setCheckpointDir()设置的。在checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移除。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。</p>
<h3 id="键值对RDD数据分区器"><a href="#键值对RDD数据分区器" class="headerlink" title="键值对RDD数据分区器"></a>键值对RDD数据分区器</h3><p>Spark目前支持Hash分区和Range分区，用户也可以自定义分区，Hash分区为当前的默认分区，Spark中分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle过程属于哪个分区和Reduce的个数</p>
<blockquote>
<p>只有Key-Value类型的RDD才有分区器的，非Key-Value类型的RDD分区器的值是None</p>
<p>每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。</p>
</blockquote>
<h4 id="Hash分区"><a href="#Hash分区" class="headerlink" title="Hash分区"></a>Hash分区</h4><p>ashPartitioner分区的原理：对于给定的key，计算其hashCode，并除以分区的个数取余，如果余数小于0，则用余数+分区的个数（否则加0），最后返回的值就是这个key所属的分区ID。</p>
<blockquote>
<p>弊端：可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有RDD的全部数据。</p>
</blockquote>
<h4 id="Ranger分区"><a href="#Ranger分区" class="headerlink" title="Ranger分区"></a>Ranger分区</h4><p>RangePartitioner作用：将一定范围内的数映射到某一个分区内，尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大，但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。</p>
<h5 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h5><ol>
<li><p>先重整个RDD中抽取出样本数据，将样本数据排序，计算出每个分区的最大key值，形成一个Array[KEY]类型的数组变量rangeBounds；</p>
</li>
<li><p>判断keyrangeBoundskey</p>
</li>
</ol>
<h4 id="自定义分区"><a href="#自定义分区" class="headerlink" title="自定义分区"></a>自定义分区</h4><p>要继承org.apache.spark.Partitioner 类并实现<code>numPartitions</code>: Int:返回创建出来的分区数。<code>getPartition(key: Any)</code>: Int:返回给定键的分区编号(0到numPartitions-1)。 </p>
<h3 id="数据读取与保存"><a href="#数据读取与保存" class="headerlink" title="数据读取与保存"></a>数据读取与保存</h3><p>文件格式分为：<strong>Text文件</strong>、<strong>Json文件</strong>、Csv文件、Sequence文件以及Object文件</p>
<p>文件系统分为：本地文件系统、<strong>HDFS</strong>、<strong>HBASE</strong>以及数据库</p>
<h4 id="Text文件"><a href="#Text文件" class="headerlink" title="Text文件"></a>Text文件</h4><p>数据读取:textFile(String)</p>
<p>数据保存:<br>saveAsTextFile(String)</p>
<h4 id="Json文件"><a href="#Json文件" class="headerlink" title="Json文件"></a>Json文件</h4><p>JSON文件中每一行就是一个JSON记录。可以通过将JSON文件当做文本文件来读取，然后利用相关的JSON库对每一条数据进行JSON解析。</p>
<blockquote>
<p>使用RDD读取JSON文件处理很复杂，同时SparkSQL集成了很好的处理JSON文件的方式，所以应用中多是采用SparkSQL处理JSON文件。</p>
</blockquote>
<ol>
<li>导入解析json所需的包</li>
</ol>
<p><code>import scala.util.parsing.json.JSON</code></p>
<ol start="2">
<li>上传json文件到HDFS</li>
</ol>
<p><code>$ hadoop fs -put ./examples/src/main/resources/people.json /</code></p>
<ol start="3">
<li>读取文件</li>
</ol>
<p><code>val json = sc.textFile(&quot;/people.json&quot;)</code></p>
<ol start="4">
<li>解析json数据</li>
</ol>
<p><code>val result  = json.map(JSON.parseFull)</code></p>
<ol start="5">
<li>打印</li>
</ol>
<p><code>result.collect</code></p>
<h3 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark_ShareData</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="keyword">val</span> dataRDD: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> i: <span class="type">Int</span> = dataRDD.reduce(_ + _)</span><br><span class="line">    println(<span class="string">"i="</span> + i) <span class="comment">//i=15</span></span><br><span class="line">    <span class="keyword">var</span> sum: <span class="type">Int</span> = <span class="number">0</span>;</span><br><span class="line">    dataRDD.foreach(i =&gt; sum += i)</span><br><span class="line">    println(<span class="string">"sum="</span> + sum) <span class="comment">//sum=0</span></span><br><span class="line">	<span class="comment">//累加器</span></span><br><span class="line">    <span class="keyword">val</span> accumulator: <span class="type">LongAccumulator</span> = sc.longAccumulator</span><br><span class="line">    dataRDD.foreach &#123;</span><br><span class="line">      j =&gt; &#123;</span><br><span class="line">        accumulator.add(j)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    println(<span class="string">"accumulator="</span> + accumulator) <span class="comment">//accumulator=LongAccumulator(id: 144, name: None, value: 15)</span></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>sum为0原理图：</p>
<p><img src="/2019/10/30/SparkCore/sum.png" alt></p>
<p>使用累加器共享变量（只写变量。可读汇总的时候读）</p>
<h4 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h4></div><div class="tags"><a href="/tags/Spark/"><i class="fa fa-tag"></i>Spark</a><a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"><i class="fa fa-tag"></i>大数据</a></div><div class="post-nav"><a class="pre" href="/2019/11/02/SparkSQL/">SparkSQL</a><a class="next" href="/2019/10/30/c++/">C++</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://example.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/%E5%B9%B6%E5%8F%91/" style="font-size: 15px;">并发</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/nosql/" style="font-size: 15px;">nosql</a> <a href="/tags/elasticsearch/" style="font-size: 15px;">elasticsearch</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/PHP/" style="font-size: 15px;">PHP</a> <a href="/tags/%E5%90%8E%E7%AB%AF/" style="font-size: 15px;">后端</a> <a href="/tags/redis/" style="font-size: 15px;">redis</a> <a href="/tags/scala/" style="font-size: 15px;">scala</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 15px;">大数据</a> <a href="/tags/SQL/" style="font-size: 15px;">SQL</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/springsecurity/" style="font-size: 15px;">springsecurity</a> <a href="/tags/php/" style="font-size: 15px;">php</a> <a href="/tags/wordpress/" style="font-size: 15px;">wordpress</a> <a href="/tags/C/" style="font-size: 15px;">C++</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/marjora/" style="font-size: 15px;">marjora</a> <a href="/tags/virtualbox/" style="font-size: 15px;">virtualbox</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/bug/" style="font-size: 15px;">bug</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 15px;">服务器</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 15px;">博客</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" style="font-size: 15px;">计算机基础</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 15px;">爬虫</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 15px;">设计模式</a> <a href="/tags/%E6%B5%81%E7%A8%8B/" style="font-size: 15px;">流程</a> <a href="/tags/%E5%BC%80%E5%8F%91/" style="font-size: 15px;">开发</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a> <a href="/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">并发编程</a> <a href="/tags/spring/" style="font-size: 15px;">spring</a> <a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 15px;">前端</a> <a href="/tags/Vue/" style="font-size: 15px;">Vue</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 15px;">数据结构</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/07/06/Redis/">Redis</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/24/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/">Git笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/07/JVM/">JVM</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/27/%E7%88%AC%E8%99%AB/">爬虫</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/">计算机组成原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/Git%E7%AC%94%E8%AE%B0/">Git笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/manjaro%E9%85%8D%E7%BD%AE%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/">manjaro配置开发环境</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/docker%E7%AC%94%E8%AE%B0/">docker笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/13/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">dian的博客.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>