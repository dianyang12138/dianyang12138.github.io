<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="后端Java工程师"><title>Spark | dian的博客</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 4.2.1"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark</h1><a id="logo" href="/.">dian的博客</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Spark</h1><div class="post-meta">2019-10-29</div><div class="post-content"><a id="more"></a>

<h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h3><p>Hadoop1.x的缺陷：</p>
<p>mr式基于数据集的计算，所以面向数据</p>
<ol>
<li>基本运算规则从存储介质中获取（采集）数据，然后进行计算，最后将结果存储到介质中，所以主要应用于一次性计算不适合于数据挖掘和机器学习和图形挖掘计算这样的迭代计算</li>
<li>MR基于文件存储介质的操作，性能非常慢</li>
<li>MR和Hadoop紧密耦合，无法动态替换</li>
</ol>
<!--more-->

<p><img src="/2019/10/29/Spark%E5%9F%BA%E7%A1%80/Hadoop1.x.png" alt></p>
<p>改进</p>
<p>由RM调度资源，AM<code>ApplicationMaster</code>调度任务。提供了一个<code>Driver</code>来于<code>Task</code>关联。RM于NM关联。AM起到Driver和RM中间人。降低耦合。<code>Contoiner</code>来协调NM和<code>Task</code></p>
<p><img src="/2019/10/29/Spark%E5%9F%BA%E7%A1%80/MR.png" alt></p>
<h3 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h3><p>13年6月份发布</p>
<p>Spark基于Hadoop1.x结构思想，采用自己的方式来改善Hadoop1.x中的问题</p>
<ol>
<li>基于内存，并且基于Scala语法开发，所以天生适合迭代式计算</li>
<li>Yran支持Spark计算</li>
</ol>
<p>Executor：计算</p>
<p>Worker：资源</p>
<blockquote>
<p>RM，NM可以替代这俩个</p>
</blockquote>
<p><img src="/2019/10/29/Spark%E5%9F%BA%E7%A1%80/Spark_work.png" alt></p>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>Spark是一种基于内存的快速、通用、可扩展的大数据分析引擎</p>
<h3 id="Spark的内置模块"><a href="#Spark的内置模块" class="headerlink" title="Spark的内置模块"></a>Spark的内置模块</h3><p><img src="/2019/10/29/Spark%E5%9F%BA%E7%A1%80/Spark_mold.png" alt></p>
<ul>
<li><p><strong>Spark Core</strong>：实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中还包含了对弹性分布式数据集(Resilient Distributed DataSet，简称RDD)的API定义。 </p>
</li>
<li><p><strong>Spark SQL</strong>：是Spark用来操作结构化数据的程序包。通过Spark SQL，我们可以使用 SQL或者Apache Hive版本的SQL方言(HQL)来查询数据。Spark SQL支持多种数据源，比如Hive表、Parquet以及JSON等。 </p>
</li>
<li><p><strong>Spark Streaming</strong>：是Spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的API，并且与Spark Core中的 RDD API高度对应。 </p>
</li>
<li><p><strong>Spark MLlib</strong>：提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。 </p>
</li>
<li><p><strong>集群管理器</strong>：Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计 算。为了实现这样的要求，同时获得最大灵活性，Spark支持在各种集群管理器(Cluster Manager)上运行，包括Hadoop YARN、Apache Mesos，以及Spark自带的一个简易调度 器，叫作独立调度器。 </p>
</li>
</ul>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li><strong>快</strong>：基于内存，效率高。Spark实现了高效的DAG执行引擎。可以通过内存处理数据流，结果也存于内存</li>
<li><strong>通用</strong>：Spark提供了统一的解决方案，可以用于批处理、交互式查询、实时流处理、机器学习、图计算。都可以无缝使用</li>
<li><strong>易用</strong>：支持Java、Python、Scala的API。还支持80多种算法。而且支持交互式的Python和Scala的Shell</li>
<li><strong>兼容性</strong>：可以使用Hadoop的Yarn和Apache Mesos作为他的资源管理和调度器。可以处理所以Hadoop支持的数据，HDFS和HBase等。</li>
</ul>
<h2 id="Spark运用"><a href="#Spark运用" class="headerlink" title="Spark运用"></a>Spark运用</h2><h3 id="Spark安装地址"><a href="#Spark安装地址" class="headerlink" title="Spark安装地址"></a>Spark安装地址</h3><p><a href="http://spark.apache.org/" target="_blank" rel="noopener">官网地址</a></p>
<p><a href="https://spark.apache.org/docs/2.1.1/" target="_blank" rel="noopener">文档查看地址</a></p>
<p><a href="https://spark.apache.org/downloads.html" target="_blank" rel="noopener">下载地址</a></p>
<h3 id="重要角色"><a href="#重要角色" class="headerlink" title="重要角色"></a>重要角色</h3><ul>
<li><p>Driver（驱动器）:Spark的驱动器是执行开发程序中的main方法的进程。它负责开发人员编写的用来创建SparkContext、创建RDD，以及进行RDD的转化操作和行动操作代码的执行。如果你是用spark shell，那么当你启动Spark shell的时候，系统后台自启了一个Spark驱动器程序，就是在Spark shell中预加载的一个叫作 sc的SparkContext对象。如果驱动器程序终止，那么Spark应用也就结束了。主要负责：</p>
<ul>
<li>把用户程序转为作业（JOB）</li>
<li>跟踪Executor的运行状况</li>
<li>为执行器节点调度任务</li>
<li>UI展示应用运行状况</li>
</ul>
<blockquote>
<p>也就是支持自定义系统开发</p>
</blockquote>
</li>
<li><p>Executor（执行器）:Spark Executor是一个工作进程，负责在 Spark 作业中运行任务，任务间相互独立。Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。主要负责：</p>
<ul>
<li>负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程；</li>
<li>通过自身的块管理器（Block Manager）为用户程序中要求缓存的RDD提供内存式存储。RDD是直接缓存在Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</li>
</ul>
<blockquote>
<p>可以有多个来执行不同任务</p>
</blockquote>
</li>
</ul>
<h3 id="Local模式"><a href="#Local模式" class="headerlink" title="Local模式"></a>Local模式</h3><h4 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h4><p>Local模式就是运行在一台计算机的模式，就是用来在本机上联手和测试。可以通过以下方式设置Master</p>
<ul>
<li>local：所有计算在一个线程中，没有并行计算</li>
<li>local[K]：K是指定几个线程来运算。通常Cpu有几个Core，就指定几个线程。</li>
<li>local[*]：帮你按照Cpu最多Cores来设置线程数</li>
</ul>
<h4 id="安装使用"><a href="#安装使用" class="headerlink" title="安装使用"></a>安装使用</h4><h5 id="上传并解压spark安装包"><a href="#上传并解压spark安装包" class="headerlink" title="上传并解压spark安装包"></a>上传并解压spark安装包</h5><p><code>[ sorfware]$ tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/</code></p>
<p><code>[ module]$ mv spark-2.1.1-bin-hadoop2.7 spark</code></p>
<p><strong>测试样例</strong></p>
<p><code>spark]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --executor-memory 1G --total-executor-cores 2 ./examples/jars/spark-examples_2.11-2.1.1.jar 100</code></p>
<h6 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class &lt;main-class&gt;</span><br><span class="line">--master &lt;master-url&gt; \</span><br><span class="line">--deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">--conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">... # other options</span><br><span class="line">&lt;application-jar&gt; \</span><br><span class="line">[application-arguments]</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<p>–master 指定Master的地址，默认为Local</p>
<p>–class: 你的应用的启动类 (如 org.apache.spark.examples.SparkPi)</p>
<p>–deploy-mode: 是否发布你的驱动到worker节点(cluster) 或者作为一个本地客户端 (client) (default: client)*</p>
<p>–conf: 任意的Spark配置属性， 格式key=value. 如果值包含空格，可以加引号“key=value” </p>
<p>application-jar: 打包好的应用jar,包含依赖. 这个URL在集群中全局可见。 比如hdfs:// 共享存储系统， 如果是 file:// path， 那么所有的节点的path都包含同样的jar</p>
<p>application-arguments: 传给main()方法的参数</p>
<p>–executor-memory 1G 指定每个executor可用内存为1G</p>
<p>–total-executor-cores 2 指定每个executor使用的cup核数为2个</p>
<blockquote>
<p>–开头的参数，可以调换位置</p>
</blockquote>
<h5 id="启动spark-shell"><a href="#启动spark-shell" class="headerlink" title="启动spark-shell"></a>启动spark-shell</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 spark]$ bin/spark-shell</span><br><span class="line">Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">Setting default log level to "WARN".</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">18/09/29 08:50:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">18/09/29 08:50:58 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</span><br><span class="line">Spark context Web UI available at http://192.168.9.102:4040</span><br><span class="line">Spark context available as 'sc' (master = local[*], app id = local-1538182253312).</span><br><span class="line">Spark session available as 'spark'.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  '_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1</span><br><span class="line">      /_/</span><br><span class="line">         </span><br><span class="line">Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br></pre></td></tr></table></figure>

<p><a href="http://192.168.9.102:4040" target="_blank" rel="noopener">UI界面</a></p>
<p>sc：可用变量    参数：master = local[*], app id = local-1538182253312</p>
<p>app id：应用id。每个应用对yarn都有一个全局的唯一Id，当前的是临时生成的</p>
<h4 id="WordCount案例"><a href="#WordCount案例" class="headerlink" title="WordCount案例"></a>WordCount案例</h4><p>在spark根目录下创建input文件夹，在内创建文件，输入单词以空格隔开</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash">sc.textFile(<span class="string">"input"</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,1)).reduceByKey(_+_).collect</span></span><br><span class="line">res0: Array[(String, Int)] = Array((hadoop,6), (oozie,3), (spark,3), (hive,3), (atguigu,3), (hbase,6))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>textFile(“input”)：input输入文件夹</p>
<p>flatMap(_.split(“ “))：以’ ‘为分隔符</p>
<p>map((_,1))：传入值为Key，1为Value</p>
<p>reduceByKey(_+_)：根据Key化简，value相加</p>
<p>collect：收集起来</p>
</blockquote>
<h4 id="Idea下编写WordCount"><a href="#Idea下编写WordCount" class="headerlink" title="Idea下编写WordCount"></a>Idea下编写WordCount</h4><p>pom文件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">finalName</span>&gt;</span>WordCount<span class="tag">&lt;/<span class="name">finalName</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--打包插件--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>WordCount<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.bigdaata.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//local模式</span></span><br><span class="line">    <span class="comment">//设置环境</span></span><br><span class="line">    <span class="comment">//1.创建SparkConf并设置App名称</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"WC"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.创建SparkContext，该对象是提交Spark App的入口</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取文件，一行一行读取</span></span><br><span class="line">    <span class="comment">//文件放在项目当前项目根目录的in文件夹内</span></span><br><span class="line">    <span class="keyword">val</span> lines: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"in"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.使用sc创建RDD并执行相应的transformation和action</span></span><br><span class="line">    <span class="keyword">val</span> words: <span class="type">RDD</span>[<span class="type">String</span>] = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="comment">//    .reduceByKey(_ + _, 1).sortBy(_._2, false).saveAsTextFile(args(1))</span></span><br><span class="line">    <span class="keyword">val</span> wordToOne: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = words.map((_, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToSum: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.reduceByKey(_ + _, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> result: <span class="type">Array</span>[(<span class="type">String</span>,<span class="type">Int</span>)] = wordToSum.collect</span><br><span class="line">    result.foreach(println)</span><br><span class="line">    <span class="comment">//4.关闭连接</span></span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="RDD简述"><a href="#RDD简述" class="headerlink" title="RDD简述"></a>RDD简述</h4><p><img src="/2019/10/29/Spark%E5%9F%BA%E7%A1%80/RDD.png" alt="img"></p>
<p>只有在collect的时候才会真正开始运行，前面都只是封装逻辑</p>
<h3 id="Yarn模式（重点）"><a href="#Yarn模式（重点）" class="headerlink" title="Yarn模式（重点）"></a>Yarn模式（重点）</h3><p>Spark客户端直接连接Yarn，不需要额外构建Spark集群。有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。</p>
<p>yarn-client：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出</p>
<p><strong>yarn-cluster</strong>：Driver程序运行在由RM（ResourceManager）启动的AP（APPMaster）适用于生产环境。</p>
<h4 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h4><p><img src="/2019/10/29/Spark%E5%9F%BA%E7%A1%80/Yarn-spark.png" alt></p>
<p><img src="/2019/10/29/Spark%E5%9F%BA%E7%A1%80/yarn-spark2.png" alt></p>
<h4 id="安装使用-1"><a href="#安装使用-1" class="headerlink" title="安装使用"></a>安装使用</h4><p>修改hadoop配置文件yarn-site.xml <code>vi yarn-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--添加--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>修改spark-env.sh <code>vi spark-env.sh</code></p>
<p><code>YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop</code></p>
<p>分发文件</p>
<p><code>xsync /opt/module/hadoop-2.7.2/etc/hadoop/yarn-site.xml</code></p>
<p><code>xsync spark-env.sh</code></p>
<p>样例测试</p>
<p>在提交任务之前需启动HDFS以及YARN集群。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure>

<p>流程图</p>
<h4 id="日志查看"><a href="#日志查看" class="headerlink" title="日志查看"></a>日志查看</h4><p>修改配置文件spark-defaults.conf </p>
<p><code>spark.yarn.historyServer.address=hadoop102:18080</code></p>
<p><code>spark.history.ui.port=18080</code></p>
<p>重启spark历史服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sbin/stop-history-server.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sbin/start-history-server.sh</span></span><br></pre></td></tr></table></figure>

<p><img src="/2019/10/29/Spark%E5%9F%BA%E7%A1%80/log_web.png" alt></p>
<h3 id="Standalone模式"><a href="#Standalone模式" class="headerlink" title="Standalone模式"></a>Standalone模式</h3><p>由Master+Slave构成的Spark集群，也就是Spark自己运行</p>
<p><img src="/2019/10/29/Spark%E5%9F%BA%E7%A1%80/Standalone-spark.png" alt></p>
<h4 id="安装使用-2"><a href="#安装使用-2" class="headerlink" title="安装使用"></a>安装使用</h4><p>在conf文件夹下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mv slaves.template slaves</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mv spark-env.sh.template spark-env.sh</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim slaves</span></span><br><span class="line"></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim spark-env.sh</span></span><br><span class="line"></span><br><span class="line">SPARK_MASTER_HOST=hadoop102</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> xsync spark/</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sbin/start-all.sh</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> xcall jps</span></span><br><span class="line">================atguigu@hadoop102================</span><br><span class="line">3330 Jps</span><br><span class="line">3238 Worker</span><br><span class="line">3163 Master</span><br><span class="line">================atguigu@hadoop103================</span><br><span class="line">2966 Jps</span><br><span class="line">2908 Worker</span><br><span class="line">================atguigu@hadoop104================</span><br><span class="line">2978 Worker</span><br><span class="line">3036 Jps</span><br></pre></td></tr></table></figure>

<p><a href="http://hadoop102:8080" target="_blank" rel="noopener">网页查看</a></p>
<blockquote>
<p>如果遇到 “JAVA_HOME not set” 异常，可以在sbin目录下的spark-config.sh 文件中</p>
<p>加入：</p>
<p>export JAVA_HOME=XXXX</p>
</blockquote>
<p>测试用例</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure>

</div><div class="tags"><a href="/tags/Spark/"><i class="fa fa-tag"></i>Spark</a><a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"><i class="fa fa-tag"></i>大数据</a></div><div class="post-nav"><a class="pre" href="/2019/10/30/c++/">C++</a><a class="next" href="/2019/10/28/Scala%E7%BC%96%E7%A8%8B/">scala函数式编程</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://example.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/%E5%B9%B6%E5%8F%91/" style="font-size: 15px;">并发</a> <a href="/tags/nosql/" style="font-size: 15px;">nosql</a> <a href="/tags/elasticsearch/" style="font-size: 15px;">elasticsearch</a> <a href="/tags/redis/" style="font-size: 15px;">redis</a> <a href="/tags/%E5%90%8E%E7%AB%AF/" style="font-size: 15px;">后端</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/PHP/" style="font-size: 15px;">PHP</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/SQL/" style="font-size: 15px;">SQL</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 15px;">大数据</a> <a href="/tags/scala/" style="font-size: 15px;">scala</a> <a href="/tags/php/" style="font-size: 15px;">php</a> <a href="/tags/wordpress/" style="font-size: 15px;">wordpress</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/springsecurity/" style="font-size: 15px;">springsecurity</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/bug/" style="font-size: 15px;">bug</a> <a href="/tags/marjora/" style="font-size: 15px;">marjora</a> <a href="/tags/virtualbox/" style="font-size: 15px;">virtualbox</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 15px;">服务器</a> <a href="/tags/C/" style="font-size: 15px;">C++</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 15px;">博客</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" style="font-size: 15px;">计算机基础</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 15px;">设计模式</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 15px;">爬虫</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">并发编程</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a> <a href="/tags/spring/" style="font-size: 15px;">spring</a> <a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 15px;">前端</a> <a href="/tags/Vue/" style="font-size: 15px;">Vue</a> <a href="/tags/%E6%B5%81%E7%A8%8B/" style="font-size: 15px;">流程</a> <a href="/tags/%E5%BC%80%E5%8F%91/" style="font-size: 15px;">开发</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 15px;">数据结构</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/07/06/Redis/">Redis</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/24/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/">Git笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/07/JVM/">JVM</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/27/%E7%88%AC%E8%99%AB/">爬虫</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/">计算机组成原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/Git%E7%AC%94%E8%AE%B0/">Git笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/manjaro%E9%85%8D%E7%BD%AE%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/">manjaro配置开发环境</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/25/docker%E7%AC%94%E8%AE%B0/">docker笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/13/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">dian的博客.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>